{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.ipynb_checkpoints',\n",
       " 'cluster.py',\n",
       " 'cl_scraper.py',\n",
       " 'craigslist_posting_therapeutic_12_12_12_23_11_18.csv',\n",
       " 'derby.log',\n",
       " 'hdfs_output.txt',\n",
       " 'km_cluster_results.npz',\n",
       " 'metastore_db',\n",
       " 'process_ads.py',\n",
       " 'single_recommender_run.sh',\n",
       " 'test.npz',\n",
       " 'Untitled.ipynb',\n",
       " '__init__.py']"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:55022)\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\opt\\spark\\spark-2.2.0-bin-hadoop2.7\\python\\lib\\py4j-0.10.4-src.zip\\py4j\\java_gateway.py\", line 963, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "  File \"C:\\Users\\shane\\Anaconda3\\envs\\py27\\lib\\socket.py\", line 228, in meth\n",
      "    return getattr(self._sock,name)(*args)\n",
      "error: [Errno 10061] No connection could be made because the target machine actively refused it\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\shane\\Anaconda3\\envs\\py27\\lib\\logging\\__init__.py\", line 872, in emit\n",
      "    stream.write(ufs % msg)\n",
      "  File \"C:\\Users\\shane\\Anaconda3\\envs\\py27\\lib\\site-packages\\ipykernel\\iostream.py\", line 360, in write\n",
      "    self._schedule_flush()\n",
      "  File \"C:\\Users\\shane\\Anaconda3\\envs\\py27\\lib\\site-packages\\ipykernel\\iostream.py\", line 309, in _schedule_flush\n",
      "    self.pub_thread.schedule(_schedule_in_thread)\n",
      "  File \"C:\\Users\\shane\\Anaconda3\\envs\\py27\\lib\\site-packages\\ipykernel\\iostream.py\", line 192, in schedule\n",
      "    f()\n",
      "  File \"C:\\Users\\shane\\Anaconda3\\envs\\py27\\lib\\site-packages\\ipykernel\\iostream.py\", line 308, in _schedule_in_thread\n",
      "    self._io_loop.call_later(self.flush_interval, self._flush)\n",
      "  File \"C:\\Users\\shane\\Anaconda3\\envs\\py27\\lib\\site-packages\\tornado\\ioloop.py\", line 520, in call_later\n",
      "    return self.call_at(self.time() + delay, callback, *args, **kwargs)\n",
      "  File \"C:\\Users\\shane\\Anaconda3\\envs\\py27\\lib\\site-packages\\tornado\\ioloop.py\", line 921, in call_at\n",
      "    heapq.heappush(self._timeouts, timeout)\n",
      "TypeError: heap argument must be a list\n",
      "Logged from file java_gateway.py, line 969\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:55022)\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\opt\\spark\\spark-2.2.0-bin-hadoop2.7\\python\\lib\\py4j-0.10.4-src.zip\\py4j\\java_gateway.py\", line 963, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "  File \"C:\\Users\\shane\\Anaconda3\\envs\\py27\\lib\\socket.py\", line 228, in meth\n",
      "    return getattr(self._sock,name)(*args)\n",
      "error: [Errno 10061] No connection could be made because the target machine actively refused it\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:55022)\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\opt\\spark\\spark-2.2.0-bin-hadoop2.7\\python\\lib\\py4j-0.10.4-src.zip\\py4j\\java_gateway.py\", line 963, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "  File \"C:\\Users\\shane\\Anaconda3\\envs\\py27\\lib\\socket.py\", line 228, in meth\n",
      "    return getattr(self._sock,name)(*args)\n",
      "error: [Errno 10061] No connection could be made because the target machine actively refused it\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\shane\\Anaconda3\\envs\\py27\\lib\\logging\\__init__.py\", line 872, in emit\n",
      "    stream.write(ufs % msg)\n",
      "  File \"C:\\Users\\shane\\Anaconda3\\envs\\py27\\lib\\site-packages\\ipykernel\\iostream.py\", line 360, in write\n",
      "    self._schedule_flush()\n",
      "  File \"C:\\Users\\shane\\Anaconda3\\envs\\py27\\lib\\site-packages\\ipykernel\\iostream.py\", line 309, in _schedule_flush\n",
      "    self.pub_thread.schedule(_schedule_in_thread)\n",
      "  File \"C:\\Users\\shane\\Anaconda3\\envs\\py27\\lib\\site-packages\\ipykernel\\iostream.py\", line 192, in schedule\n",
      "    f()\n",
      "  File \"C:\\Users\\shane\\Anaconda3\\envs\\py27\\lib\\site-packages\\ipykernel\\iostream.py\", line 308, in _schedule_in_thread\n",
      "    self._io_loop.call_later(self.flush_interval, self._flush)\n",
      "  File \"C:\\Users\\shane\\Anaconda3\\envs\\py27\\lib\\site-packages\\tornado\\ioloop.py\", line 520, in call_later\n",
      "    return self.call_at(self.time() + delay, callback, *args, **kwargs)\n",
      "  File \"C:\\Users\\shane\\Anaconda3\\envs\\py27\\lib\\site-packages\\tornado\\ioloop.py\", line 921, in call_at\n",
      "    heapq.heappush(self._timeouts, timeout)\n",
      "TypeError: heap argument must be a list\n",
      "Logged from file java_gateway.py, line 969\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:55022)\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\opt\\spark\\spark-2.2.0-bin-hadoop2.7\\python\\lib\\py4j-0.10.4-src.zip\\py4j\\java_gateway.py\", line 963, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "  File \"C:\\Users\\shane\\Anaconda3\\envs\\py27\\lib\\socket.py\", line 228, in meth\n",
      "    return getattr(self._sock,name)(*args)\n",
      "error: [Errno 10061] No connection could be made because the target machine actively refused it\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:55022)\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\opt\\spark\\spark-2.2.0-bin-hadoop2.7\\python\\lib\\py4j-0.10.4-src.zip\\py4j\\java_gateway.py\", line 963, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "  File \"C:\\Users\\shane\\Anaconda3\\envs\\py27\\lib\\socket.py\", line 228, in meth\n",
      "    return getattr(self._sock,name)(*args)\n",
      "error: [Errno 10061] No connection could be made because the target machine actively refused it\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\shane\\Anaconda3\\envs\\py27\\lib\\logging\\__init__.py\", line 872, in emit\n",
      "    stream.write(ufs % msg)\n",
      "  File \"C:\\Users\\shane\\Anaconda3\\envs\\py27\\lib\\site-packages\\ipykernel\\iostream.py\", line 360, in write\n",
      "    self._schedule_flush()\n",
      "  File \"C:\\Users\\shane\\Anaconda3\\envs\\py27\\lib\\site-packages\\ipykernel\\iostream.py\", line 309, in _schedule_flush\n",
      "    self.pub_thread.schedule(_schedule_in_thread)\n",
      "  File \"C:\\Users\\shane\\Anaconda3\\envs\\py27\\lib\\site-packages\\ipykernel\\iostream.py\", line 192, in schedule\n",
      "    f()\n",
      "  File \"C:\\Users\\shane\\Anaconda3\\envs\\py27\\lib\\site-packages\\ipykernel\\iostream.py\", line 308, in _schedule_in_thread\n",
      "    self._io_loop.call_later(self.flush_interval, self._flush)\n",
      "  File \"C:\\Users\\shane\\Anaconda3\\envs\\py27\\lib\\site-packages\\tornado\\ioloop.py\", line 520, in call_later\n",
      "    return self.call_at(self.time() + delay, callback, *args, **kwargs)\n",
      "  File \"C:\\Users\\shane\\Anaconda3\\envs\\py27\\lib\\site-packages\\tornado\\ioloop.py\", line 921, in call_at\n",
      "    heapq.heappush(self._timeouts, timeout)\n",
      "TypeError: heap argument must be a list\n",
      "Logged from file java_gateway.py, line 969\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:55022)\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\opt\\spark\\spark-2.2.0-bin-hadoop2.7\\python\\lib\\py4j-0.10.4-src.zip\\py4j\\java_gateway.py\", line 963, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "  File \"C:\\Users\\shane\\Anaconda3\\envs\\py27\\lib\\socket.py\", line 228, in meth\n",
      "    return getattr(self._sock,name)(*args)\n",
      "error: [Errno 10061] No connection could be made because the target machine actively refused it\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:55022)\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\opt\\spark\\spark-2.2.0-bin-hadoop2.7\\python\\lib\\py4j-0.10.4-src.zip\\py4j\\java_gateway.py\", line 963, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "  File \"C:\\Users\\shane\\Anaconda3\\envs\\py27\\lib\\socket.py\", line 228, in meth\n",
      "    return getattr(self._sock,name)(*args)\n",
      "error: [Errno 10061] No connection could be made because the target machine actively refused it\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\shane\\Anaconda3\\envs\\py27\\lib\\logging\\__init__.py\", line 872, in emit\n",
      "    stream.write(ufs % msg)\n",
      "  File \"C:\\Users\\shane\\Anaconda3\\envs\\py27\\lib\\site-packages\\ipykernel\\iostream.py\", line 360, in write\n",
      "    self._schedule_flush()\n",
      "  File \"C:\\Users\\shane\\Anaconda3\\envs\\py27\\lib\\site-packages\\ipykernel\\iostream.py\", line 309, in _schedule_flush\n",
      "    self.pub_thread.schedule(_schedule_in_thread)\n",
      "  File \"C:\\Users\\shane\\Anaconda3\\envs\\py27\\lib\\site-packages\\ipykernel\\iostream.py\", line 192, in schedule\n",
      "    f()\n",
      "  File \"C:\\Users\\shane\\Anaconda3\\envs\\py27\\lib\\site-packages\\ipykernel\\iostream.py\", line 308, in _schedule_in_thread\n",
      "    self._io_loop.call_later(self.flush_interval, self._flush)\n",
      "  File \"C:\\Users\\shane\\Anaconda3\\envs\\py27\\lib\\site-packages\\tornado\\ioloop.py\", line 520, in call_later\n",
      "    return self.call_at(self.time() + delay, callback, *args, **kwargs)\n",
      "  File \"C:\\Users\\shane\\Anaconda3\\envs\\py27\\lib\\site-packages\\tornado\\ioloop.py\", line 921, in call_at\n",
      "    heapq.heappush(self._timeouts, timeout)\n",
      "TypeError: heap argument must be a list\n",
      "Logged from file java_gateway.py, line 969\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:55022)\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\opt\\spark\\spark-2.2.0-bin-hadoop2.7\\python\\lib\\py4j-0.10.4-src.zip\\py4j\\java_gateway.py\", line 963, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "  File \"C:\\Users\\shane\\Anaconda3\\envs\\py27\\lib\\socket.py\", line 228, in meth\n",
      "    return getattr(self._sock,name)(*args)\n",
      "error: [Errno 10061] No connection could be made because the target machine actively refused it\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:55022)\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\opt\\spark\\spark-2.2.0-bin-hadoop2.7\\python\\lib\\py4j-0.10.4-src.zip\\py4j\\java_gateway.py\", line 963, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "  File \"C:\\Users\\shane\\Anaconda3\\envs\\py27\\lib\\socket.py\", line 228, in meth\n",
      "    return getattr(self._sock,name)(*args)\n",
      "error: [Errno 10061] No connection could be made because the target machine actively refused it\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\shane\\Anaconda3\\envs\\py27\\lib\\logging\\__init__.py\", line 872, in emit\n",
      "    stream.write(ufs % msg)\n",
      "  File \"C:\\Users\\shane\\Anaconda3\\envs\\py27\\lib\\site-packages\\ipykernel\\iostream.py\", line 360, in write\n",
      "    self._schedule_flush()\n",
      "  File \"C:\\Users\\shane\\Anaconda3\\envs\\py27\\lib\\site-packages\\ipykernel\\iostream.py\", line 309, in _schedule_flush\n",
      "    self.pub_thread.schedule(_schedule_in_thread)\n",
      "  File \"C:\\Users\\shane\\Anaconda3\\envs\\py27\\lib\\site-packages\\ipykernel\\iostream.py\", line 192, in schedule\n",
      "    f()\n",
      "  File \"C:\\Users\\shane\\Anaconda3\\envs\\py27\\lib\\site-packages\\ipykernel\\iostream.py\", line 308, in _schedule_in_thread\n",
      "    self._io_loop.call_later(self.flush_interval, self._flush)\n",
      "  File \"C:\\Users\\shane\\Anaconda3\\envs\\py27\\lib\\site-packages\\tornado\\ioloop.py\", line 520, in call_later\n",
      "    return self.call_at(self.time() + delay, callback, *args, **kwargs)\n",
      "  File \"C:\\Users\\shane\\Anaconda3\\envs\\py27\\lib\\site-packages\\tornado\\ioloop.py\", line 921, in call_at\n",
      "    heapq.heappush(self._timeouts, timeout)\n",
      "TypeError: heap argument must be a list\n",
      "Logged from file java_gateway.py, line 969\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:55022)\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\opt\\spark\\spark-2.2.0-bin-hadoop2.7\\python\\lib\\py4j-0.10.4-src.zip\\py4j\\java_gateway.py\", line 963, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "  File \"C:\\Users\\shane\\Anaconda3\\envs\\py27\\lib\\socket.py\", line 228, in meth\n",
      "    return getattr(self._sock,name)(*args)\n",
      "error: [Errno 10061] No connection could be made because the target machine actively refused it\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:55022)\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\opt\\spark\\spark-2.2.0-bin-hadoop2.7\\python\\lib\\py4j-0.10.4-src.zip\\py4j\\java_gateway.py\", line 963, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "  File \"C:\\Users\\shane\\Anaconda3\\envs\\py27\\lib\\socket.py\", line 228, in meth\n",
      "    return getattr(self._sock,name)(*args)\n",
      "error: [Errno 10061] No connection could be made because the target machine actively refused it\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\shane\\Anaconda3\\envs\\py27\\lib\\logging\\__init__.py\", line 872, in emit\n",
      "    stream.write(ufs % msg)\n",
      "  File \"C:\\Users\\shane\\Anaconda3\\envs\\py27\\lib\\site-packages\\ipykernel\\iostream.py\", line 360, in write\n",
      "    self._schedule_flush()\n",
      "  File \"C:\\Users\\shane\\Anaconda3\\envs\\py27\\lib\\site-packages\\ipykernel\\iostream.py\", line 309, in _schedule_flush\n",
      "    self.pub_thread.schedule(_schedule_in_thread)\n",
      "  File \"C:\\Users\\shane\\Anaconda3\\envs\\py27\\lib\\site-packages\\ipykernel\\iostream.py\", line 192, in schedule\n",
      "    f()\n",
      "  File \"C:\\Users\\shane\\Anaconda3\\envs\\py27\\lib\\site-packages\\ipykernel\\iostream.py\", line 308, in _schedule_in_thread\n",
      "    self._io_loop.call_later(self.flush_interval, self._flush)\n",
      "  File \"C:\\Users\\shane\\Anaconda3\\envs\\py27\\lib\\site-packages\\tornado\\ioloop.py\", line 520, in call_later\n",
      "    return self.call_at(self.time() + delay, callback, *args, **kwargs)\n",
      "  File \"C:\\Users\\shane\\Anaconda3\\envs\\py27\\lib\\site-packages\\tornado\\ioloop.py\", line 921, in call_at\n",
      "    heapq.heappush(self._timeouts, timeout)\n",
      "TypeError: heap argument must be a list\n",
      "Logged from file java_gateway.py, line 969\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:55022)\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\opt\\spark\\spark-2.2.0-bin-hadoop2.7\\python\\lib\\py4j-0.10.4-src.zip\\py4j\\java_gateway.py\", line 963, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "  File \"C:\\Users\\shane\\Anaconda3\\envs\\py27\\lib\\socket.py\", line 228, in meth\n",
      "    return getattr(self._sock,name)(*args)\n",
      "error: [Errno 10061] No connection could be made because the target machine actively refused it\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:55022)\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\opt\\spark\\spark-2.2.0-bin-hadoop2.7\\python\\lib\\py4j-0.10.4-src.zip\\py4j\\java_gateway.py\", line 963, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "  File \"C:\\Users\\shane\\Anaconda3\\envs\\py27\\lib\\socket.py\", line 228, in meth\n",
      "    return getattr(self._sock,name)(*args)\n",
      "error: [Errno 10061] No connection could be made because the target machine actively refused it\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\shane\\Anaconda3\\envs\\py27\\lib\\logging\\__init__.py\", line 872, in emit\n",
      "    stream.write(ufs % msg)\n",
      "  File \"C:\\Users\\shane\\Anaconda3\\envs\\py27\\lib\\site-packages\\ipykernel\\iostream.py\", line 360, in write\n",
      "    self._schedule_flush()\n",
      "  File \"C:\\Users\\shane\\Anaconda3\\envs\\py27\\lib\\site-packages\\ipykernel\\iostream.py\", line 309, in _schedule_flush\n",
      "    self.pub_thread.schedule(_schedule_in_thread)\n",
      "  File \"C:\\Users\\shane\\Anaconda3\\envs\\py27\\lib\\site-packages\\ipykernel\\iostream.py\", line 192, in schedule\n",
      "    f()\n",
      "  File \"C:\\Users\\shane\\Anaconda3\\envs\\py27\\lib\\site-packages\\ipykernel\\iostream.py\", line 308, in _schedule_in_thread\n",
      "    self._io_loop.call_later(self.flush_interval, self._flush)\n",
      "  File \"C:\\Users\\shane\\Anaconda3\\envs\\py27\\lib\\site-packages\\tornado\\ioloop.py\", line 520, in call_later\n",
      "    return self.call_at(self.time() + delay, callback, *args, **kwargs)\n",
      "  File \"C:\\Users\\shane\\Anaconda3\\envs\\py27\\lib\\site-packages\\tornado\\ioloop.py\", line 921, in call_at\n",
      "    heapq.heappush(self._timeouts, timeout)\n",
      "TypeError: heap argument must be a list\n",
      "Logged from file java_gateway.py, line 969\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:55022)\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\opt\\spark\\spark-2.2.0-bin-hadoop2.7\\python\\lib\\py4j-0.10.4-src.zip\\py4j\\java_gateway.py\", line 963, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "  File \"C:\\Users\\shane\\Anaconda3\\envs\\py27\\lib\\socket.py\", line 228, in meth\n",
      "    return getattr(self._sock,name)(*args)\n",
      "error: [Errno 10061] No connection could be made because the target machine actively refused it\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.listdir(os.curdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import mllib\n",
    "from pyspark.mllib.evaluation import RegressionMetrics\n",
    "from pyspark.mllib.feature import HashingTF, IDF\n",
    "from pyspark.mllib.feature import Word2Vec\n",
    "from pyspark.mllib.feature import StandardScalerModel\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.linalg.distributed import RowMatrix, BlockMatrix\n",
    "from pyspark.mllib.linalg import Matrices\n",
    "from pyspark.mllib.clustering import KMeans, KMeansModel\n",
    "\n",
    "from math import sqrt\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "import os, csv, sys, time\n",
    "from random import randint\n",
    "from itertools import izip, izip_longest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shane\\programming\\cs657_mining_massive_datasets\\craigslist_clustering\\data\\cl_tiny.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# def get_input(fn):\n",
    "\n",
    "fn = \"cl_tiny.csv\"\n",
    "cur_dir = os.path.abspath(os.curdir)\n",
    "input_file_path = os.path.normpath(os.path.join(cur_dir, \"..\", \"data\", fn))\n",
    "print(input_file_path)\n",
    "\n",
    "os.path.isfile(input_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# [(postTitle, postingURL, postLocation, time, lat, long, address, dateRetrieved, post_date, ad), ...]\n",
    "# tiny input has 30 reviews\n",
    "raw_ads = sc.textFile(input_file_path)\n",
    "# set = input.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# [ad0, ad1, ..]\n",
    "ads_rdd = raw_ads.map(lambda x: str(x.decode('utf-8', 'ignore')))\n",
    "# processed_rdd = input.map(lambda x: str(x.decode('utf-8', 'ignore'))).map(lambda x: x.split(\",\"))\n",
    "# processed_rdd.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tfidf_rdd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-9f6278e40f00>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# tfidf_rdd.flatMap(lambda x: x).take(3)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# flatMap by keeping the column position\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mflat_rdd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtfidf_rdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatMap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mcol\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindexOf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mflat_rdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# .map(v => (v._2, v._1)) // key by column position\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tfidf_rdd' is not defined"
     ]
    }
   ],
   "source": [
    "# def transpose_rdd()\n",
    "# tfidf_rdd.flatMap(lambda x: x).take(3)\n",
    "# flatMap by keeping the column position\n",
    "flat_rdd = tfidf_rdd.flatMap(lambda row: row.map(lambda col: (col, row.indexOf(col))))\n",
    "flat_rdd.take(3)\n",
    "# .map(v => (v._2, v._1)) // key by column position\n",
    "# .groupByKey.sortByKey   // regroup on column position, thus all elements from the first column will be in the first row\n",
    "# .map(_._2)              // discard the key, keep only value\n",
    "# df = rdd.toDF()\n",
    "# # Grab data from first columns, since it will be transposed to new column headers\n",
    "# new_header = [i[0] for i in dt.select(\"_1\").rdd.map(tuple).collect()]\n",
    "\n",
    "# # Remove first column from dataframe\n",
    "# dt2 = dt.select([c for c in dt.columns if c not in ['_1']])\n",
    "\n",
    "# # Convert DataFrame to RDD\n",
    "# rdd = dt2.rdd.map(tuple)\n",
    "\n",
    "# # Transpose Data\n",
    "# rddT1 = rdd.zipWithIndex().flatMap(lambda (x,i): [(i,j,e) for (j,e) in enumerate(x)])\n",
    "# rddT2 = rddT1.map(lambda (i,j,e): (j, (i,e))).groupByKey().sortByKey()\n",
    "# rddT3 = rddT2.map(lambda (i, x): sorted(list(x), cmp=lambda (i1,e1),(i2,e2) : cmp(i1, i2)))\n",
    "# rddT4 = rddT3.map(lambda x: map(lambda (i, y): y , x))\n",
    "\n",
    "# # Convert back to DataFrame (along with header)\n",
    "# df = rddT4.toDF(new_header)\n",
    "\n",
    "# return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_tfidf(rdd):\n",
    "    # While applying HashingTF only needs a single pass to the data, applying IDF needs two passes:\n",
    "    # First to compute the IDF vector and second to scale the term frequencies by IDF.\n",
    "    hashingTF = HashingTF()\n",
    "    tf = hashingTF.transform(rdd)\n",
    "    tf.cache()\n",
    "    idf = IDF().fit(tf)\n",
    "    tfidf = idf.transform(tf)\n",
    "    # spark.mllib's IDF implementation provides an option for ignoring terms\n",
    "    # which occur in less than a minimum number of documents.\n",
    "    # In such cases, the IDF for these terms is set to 0.\n",
    "    # This feature can be used by passing the minDocFreq value to the IDF constructor.\n",
    "    idfIgnore = IDF(minDocFreq=1).fit(tf)\n",
    "    tfidf_rdd = idfIgnore.transform(tf)\n",
    "    # rdd of SparseVectors [(doc_id_i: {word_id_j: tfidfscore_j, ...}), ... }]\n",
    "    # or m docs x n counts\n",
    "    return tfidf_rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_svd_U(tfidf_rdd, n_topics=3):\n",
    "    # distributed matrix\n",
    "    matrix_rdd = RowMatrix(tfidf_rdd)\n",
    "\n",
    "    matrix_rdd.numRows\n",
    "    # matrix_rdd.rows.take(3)\n",
    "    svd = matrix_rdd.computeSVD(3, computeU=True)\n",
    "    \n",
    "    # left singular vectors\n",
    "    # type = RowMatrix\n",
    "    svd_u = svd.U\n",
    "    \n",
    "    # array of DenseVectors, m_documents x n_topics\n",
    "    # [[topic_i, ...], ...]\n",
    "    return svd_u.rows.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_svd_U(svd_U_rdd, fn=\"svd_u_results.npz\"):\n",
    "    np.savez(fn,np.array(svd_U_rdd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf_rdd = get_tfidf(ads_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#     sentence = \"aa bb ab\" * 10 + \"a cd \" * 10\n",
    "#     localDoc = [sentence, sentence]\n",
    "#     doc = sc.parallelize(localDoc).map(lambda line: line.split(\" \"))\n",
    "#     model = Word2Vec().setVectorSize(10).setSeed(42).fit(doc)\n",
    "# i think it is expecting a list of document lists [[word1, word2,...], ...]\n",
    "def get_word2vec(rdd):\n",
    "    word2vec = Word2Vec()\n",
    "    model = word2vec.fit(ads_rdd)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k_means\n",
    "# Build the model (cluster the data)\n",
    "# kmeans(rdd, k, maxIterations, runs, InitializationMode, seed, initializationSteps, epsilon, initialModel)\n",
    "def cluster_kmeans_svd(svd_u, k=2, n_iters=10, save_model=False):\n",
    "    rdd = svd.U.rows\n",
    "    clusters = KMeans.train(rdd, k, maxIterations=n_iters, initializationMode=\"random\")\n",
    "    WSSSE = rdd.map(lambda point: error(point)).reduce(lambda x, y: x + y)\n",
    "    print(\"Within Set Sum of Squared Error = \" + str(WSSSE))\n",
    "    if save_model:\n",
    "        save_cluster_model(clusters, fn)\n",
    "    \n",
    "    # do i need to keep the predictions with the keys?\n",
    "    # returns a list of labels\n",
    "    return rdd.map(lambda x: ((x[0],x[1]), clusters.predict(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_cluster_predictions(labels, fn=\"test_results.pkl\"):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Evaluate clustering by computing Within Set Sum of Squared Errors\n",
    "def error(point):\n",
    "    center = clusters.centers[clusters.predict(point)]\n",
    "    return sqrt(sum([x**2 for x in (point - center)]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Within Set Sum of Squared Error = 5.32034738594\n"
     ]
    }
   ],
   "source": [
    "kmeans_mapped = cluster_kmeans_svd(svd.U)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(\"km_cluster_results.npz\", kmeans_mapped.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save and load model\n",
    "def save_cluster_model(clusters, fn=\"test_model\"):\n",
    "    clusters.save(sc, \"target/org/apache/spark/PythonKMeansExample/KMeansModel\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cluster_model(clusters, fn=\"test_model\"):\n",
    "    sameModel = KMeansModel.load(sc, fn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
