{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# general pyspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n",
    "\n",
    "# conf = SparkConf().setMaster(\"local\").setAppName(\"svd_cluster.py\")\n",
    "# sc = SparkContext(conf = conf)\n",
    "\n",
    "#import mllib\n",
    "from pyspark.mllib.evaluation import RegressionMetrics\n",
    "from pyspark.mllib.feature import HashingTF, IDF\n",
    "from pyspark.mllib.feature import Word2Vec\n",
    "from pyspark.mllib.feature import StandardScalerModel\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.linalg.distributed import RowMatrix, BlockMatrix\n",
    "from pyspark.mllib.linalg import Matrices\n",
    "from pyspark.mllib.clustering import KMeans, KMeansModel\n",
    "from pyspark.mllib.clustering import LDA, LDAModel\n",
    "from pyspark.mllib.clustering import BisectingKMeans, BisectingKMeansModel\n",
    "\n",
    "# python imports\n",
    "from math import sqrt\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "import os, csv, sys, time\n",
    "from random import randint\n",
    "from itertools import izip, izip_longest\n",
    "import string\n",
    "import translitcodec\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "sc = SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_tfidf(rdd, verbose=True):\n",
    "    if verbose: print(\"in get_tfidf\")\n",
    "    # While applying HashingTF only needs a single pass to the data, applying IDF needs two passes:\n",
    "    # First to compute the IDF vector and second to scale the term frequencies by IDF.\n",
    "    hashingTF = HashingTF()\n",
    "    tf = hashingTF.transform(rdd)\n",
    "    tf.cache()\n",
    "    idf = IDF().fit(tf)\n",
    "    tfidf = idf.transform(tf)\n",
    "    # spark.mllib's IDF implementation provides an option for ignoring terms\n",
    "    # which occur in less than a minimum number of documents.\n",
    "    # In such cases, the IDF for these terms is set to 0.\n",
    "    # This feature can be used by passing the minDocFreq value to the IDF constructor.\n",
    "    idfIgnore = IDF(minDocFreq=1).fit(tf)\n",
    "    tfidf_rdd = idfIgnore.transform(tf)\n",
    "    # rdd of SparseVectors [(doc_id_i: {word_id_j: tfidfscore_j, ...}), ... }]\n",
    "    # or m docs x n counts\n",
    "    return tfidf_rdd\n",
    "\n",
    "def get_svd(tfidf_rdd, n_topics=3, verbose=True):\n",
    "    if verbose: print(\"in get_svd\")\n",
    "    # distributed matrix\n",
    "    matrix_rdd = RowMatrix(tfidf_rdd)\n",
    "#     left singular vectors\n",
    "#     type = RowMatrix\n",
    "#     svd_u = svd.U\n",
    "#     array of DenseVectors, m_documents x n_topics\n",
    "#     [[topic_i, ...], ...]\n",
    "#     return svd_u.rows.collect()\n",
    "    svd = matrix_rdd.computeSVD(n_topics, computeU=True)\n",
    "    return svd\n",
    "\n",
    "def save_svd_U(svd_U_rdd, fn=\"svd_u_results.npz\"):\n",
    "    np.savez(fn,np.array(svd_U_rdd))\n",
    "\n",
    "#     sentence = \"aa bb ab\" * 10 + \"a cd \" * 10\n",
    "#     localDoc = [sentence, sentence]\n",
    "#     doc = sc.parallelize(localDoc).map(lambda line: line.split(\" \"))\n",
    "#     model = Word2Vec().setVectorSize(10).setSeed(42).fit(doc)save_cluster_metrics\n",
    "# i think it is expecting a list of document lists [[word1, word2,...], ...]\n",
    "def get_word2vec(rdd):\n",
    "    word2vec = Word2Vec()\n",
    "    model = word2vec.fit(ads_rdd)\n",
    "\n",
    "def save_cluster_predictions(cluster_results, model=\"km\", fn=\"cluster_results.pkl\"):\n",
    "    results_fn = \"{}_{}\".format(model, fn)\n",
    "    np.savez(results_fn, cluster_results)\n",
    "\n",
    "# Evaluate clustering by computing Within Set Sum of Squared Errors\n",
    "def error(point):\n",
    "    center = clusters.centers[clusters.predict(point)]\n",
    "    return sqrt(sum([x**2 for x in (point - center)]))\n",
    "\n",
    "\n",
    "# Save and load model\n",
    "def save_cluster_model(clusters, fn=\"test_model\"):\n",
    "    clusters.save(sc, \"target/org/apache/spark/PythonKMeansExample/KMeansModel\")\n",
    "\n",
    "def load_cluster_model(clusters, fn=\"test_model\"):\n",
    "    sameModel = KMeansModel.load(sc, fn)\n",
    "\n",
    "\n",
    "def save_cluster_metrics(model, score, svd_dims, k=None, max_iters=None, clust_size=None, doc_concept=None, topic_concept=None):\n",
    "    print('saving cluster metrics to csv')\n",
    "    row = [model, score, svd_dims, k, max_iters, clust_size, doc_concept, topic_concept]\n",
    "    with open(fn, 'a+') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shane\\programming\\cs657_mining_massive_datasets\\craigslist_clustering\\data\\cl_ads_all.csv\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "#------------read in data\n",
    "\n",
    "# tiny input has 30 reviews\n",
    "# read in input from csv\n",
    "# fn = \"cl_tiny.csv\"\n",
    "\n",
    "# fn = \"cl_combined.csv\"\n",
    "fn = \"cl_ads_all.csv\"\n",
    "cur_dir = os.path.abspath(os.curdir)\n",
    "input_file_path = os.path.normpath(os.path.join(cur_dir, \"..\", \"data\", fn))\n",
    "print(input_file_path)\n",
    "print(os.path.isfile(input_file_path))\n",
    "\n",
    "# combined.csv = [(postTitle, postingURL, postLocation, time, lat, long, address, dateRetrieved, post_date, ad), ...]\n",
    "# ads_all.csv = [(id, listing_type, ad), ...]\n",
    "raw_ads = sc.textFile(input_file_path)\n",
    "header = raw_ads.first()\n",
    "# take out header\n",
    "raw_ads = raw_ads.filter(lambda line: line != header)\n",
    "# raw_ads.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[u'1', u'thp', u'\"Daziran Massage TherapyCall to make appointment.1314 Westgate Parkway, Suite 4 Dothan, AlabamaWe specialize in Back Walking, Deep Tissue, Sport Massage...etc$60/hour $65/hour deep tissue, we also have 90 Mins and 2 hours session.out call only within Dothan area.Open Monday to Saturday, 10am - 9pm, Sunday 1pm - 8pmshow contact infoAll major credit cards accepted    ***Daziran Massage***334-446-3721\",,,,,,,,,,,,,,,,,,,'], [u'2', u'thp', u'\"331 SPAWelcome to best asian massage. shiatsu and swedish, walk on your back , firm or relaxing massage.Great table shower.Our place is very clean and very comfortable.Every therapists are licensed and very professional and friendly.You\\'ll be a New person,In callHalf Hr: $60.00 with table showerOne Hr: $80.00 with table showerwe do accept credit cardsopen 9 am to 10 pm , 7days a weekTel:  show contact info*from panama city or beach: one mile before to I-10 on highway 331 to your left* from Tallahassee: Left turn to highway 331 (Exit 85) , we are 1 mile from I-10 on your right side building.*I-10 from alabama(Mobil) or Pesacola Fl - right turn to highway 331 and 1mile to your right.    ASIAN PROFESSIONAL MASSAGE \"\"right off, I-10 on 331 S\"\"\",,,,,,,,,,,,,,,,,,,'], [u'3', u'thp', u\"Hi I'm Kali and i have the most wonderful relaxing body rubs in town...........pls call or text me for the rates    *******HEAVENLY BODY********,,,,,,,,,,,,,,,,,,,\"], [u'4', u'thp', u'\"JOUN SPA.................We do Shiatsu, Swedish & Deep-tissue Combination massageYou\\'ll receive best quality massage in town.Half Hr: $60.00One Hr: $80.00 Include Table Showeropen 9am to 10pm We have additional parkings are back of the building and use the back entrance, pleaseOur location is between 23rd st and airport rd, behind the \"\"Dunkin Donuts\"\"\\'Call:  show contact infoMM 37226credit card OK    \"\"GRAND OPENING\"\" ASIAN MASAGE\",,,,,,,,,,,,,,,,,,,'], [u'5', u'thp', u'\"331 SPAWelcome to best asian massage, Shiatsu and Swedish, Walk on your back, firm or relaxing massage.Great table showerOur place is very clea and very comfortable.Every therapists are licensed and very professional and friendly.You\\'ll be a New Person.Half Hr : $60.00One  Hr:   $80.00Open 9 am to 10 pm, 7days a weekCall:  show contact info*from the Panama city,  beach :   one mile before the I-10 on highway 331 to your left*from the Tallahassee & Mariana :           I-10 Exit 85,   left turn to 331 one mile on your right*from the  Mobil ,Crestview, Pensacola : I-10 Exit 85, right turn to 331 one mile on your right    ASIAN PROFESSIONAL MASSAGE \"\"right off, I-10 on 331 S\"\"\",,,,,,,,,,,,,,,,,,,']]\n",
      "[((u'1', u'thp'), u'\"Daziran Massage TherapyCall to make appointment.1314 Westgate Parkway, Suite 4 Dothan, AlabamaWe specialize in Back Walking, Deep Tissue, Sport Massage...etc$60/hour $65/hour deep tissue, we also have 90 Mins and 2 hours session.out call only within Dothan area.Open Monday to Saturday, 10am - 9pm, Sunday 1pm - 8pmshow contact infoAll major credit cards accepted    ***Daziran Massage***334-446-3721\",,,,,,,,,,,,,,,,,,,'), ((u'2', u'thp'), u'\"331 SPAWelcome to best asian massage. shiatsu and swedish, walk on your back , firm or relaxing massage.Great table shower.Our place is very clean and very comfortable.Every therapists are licensed and very professional and friendly.You\\'ll be a New person,In callHalf Hr: $60.00 with table showerOne Hr: $80.00 with table showerwe do accept credit cardsopen 9 am to 10 pm , 7days a weekTel:  show contact info*from panama city or beach: one mile before to I-10 on highway 331 to your left* from Tallahassee: Left turn to highway 331 (Exit 85) , we are 1 mile from I-10 on your right side building.*I-10 from alabama(Mobil) or Pesacola Fl - right turn to highway 331 and 1mile to your right.    ASIAN PROFESSIONAL MASSAGE \"\"right off, I-10 on 331 S\"\"\",,,,,,,,,,,,,,,,,,,'), ((u'3', u'thp'), u\"Hi I'm Kali and i have the most wonderful relaxing body rubs in town...........pls call or text me for the rates    *******HEAVENLY BODY********,,,,,,,,,,,,,,,,,,,\"), ((u'4', u'thp'), u'\"JOUN SPA.................We do Shiatsu, Swedish & Deep-tissue Combination massageYou\\'ll receive best quality massage in town.Half Hr: $60.00One Hr: $80.00 Include Table Showeropen 9am to 10pm We have additional parkings are back of the building and use the back entrance, pleaseOur location is between 23rd st and airport rd, behind the \"\"Dunkin Donuts\"\"\\'Call:  show contact infoMM 37226credit card OK    \"\"GRAND OPENING\"\" ASIAN MASAGE\",,,,,,,,,,,,,,,,,,,'), ((u'5', u'thp'), u'\"331 SPAWelcome to best asian massage, Shiatsu and Swedish, Walk on your back, firm or relaxing massage.Great table showerOur place is very clea and very comfortable.Every therapists are licensed and very professional and friendly.You\\'ll be a New Person.Half Hr : $60.00One  Hr:   $80.00Open 9 am to 10 pm, 7days a weekCall:  show contact info*from the Panama city,  beach :   one mile before the I-10 on highway 331 to your left*from the Tallahassee & Mariana :           I-10 Exit 85,   left turn to 331 one mile on your right*from the  Mobil ,Crestview, Pensacola : I-10 Exit 85, right turn to 331 one mile on your right    ASIAN PROFESSIONAL MASSAGE \"\"right off, I-10 on 331 S\"\"\",,,,,,,,,,,,,,,,,,,')]\n",
      "[((u'1', u'thp'), '\"Daziran Massage TherapyCall to make appointment.1314 Westgate Parkway, Suite 4 Dothan, AlabamaWe specialize in Back Walking, Deep Tissue, Sport Massage...etc$60/hour $65/hour deep tissue, we also have 90 Mins and 2 hours session.out call only within Dothan area.Open Monday to Saturday, 10am - 9pm, Sunday 1pm - 8pmshow contact infoAll major credit cards accepted    ***Daziran Massage***334-446-3721\",,,,,,,,,,,,,,,,,,,'), ((u'2', u'thp'), '\"331 SPAWelcome to best asian massage. shiatsu and swedish, walk on your back , firm or relaxing massage.Great table shower.Our place is very clean and very comfortable.Every therapists are licensed and very professional and friendly.You\\'ll be a New person,In callHalf Hr: $60.00 with table showerOne Hr: $80.00 with table showerwe do accept credit cardsopen 9 am to 10 pm , 7days a weekTel:  show contact info*from panama city or beach: one mile before to I-10 on highway 331 to your left* from Tallahassee: Left turn to highway 331 (Exit 85) , we are 1 mile from I-10 on your right side building.*I-10 from alabama(Mobil) or Pesacola Fl - right turn to highway 331 and 1mile to your right.    ASIAN PROFESSIONAL MASSAGE \"\"right off, I-10 on 331 S\"\"\",,,,,,,,,,,,,,,,,,,'), ((u'3', u'thp'), \"Hi I'm Kali and i have the most wonderful relaxing body rubs in town...........pls call or text me for the rates    *******HEAVENLY BODY********,,,,,,,,,,,,,,,,,,,\"), ((u'4', u'thp'), '\"JOUN SPA.................We do Shiatsu, Swedish & Deep-tissue Combination massageYou\\'ll receive best quality massage in town.Half Hr: $60.00One Hr: $80.00 Include Table Showeropen 9am to 10pm We have additional parkings are back of the building and use the back entrance, pleaseOur location is between 23rd st and airport rd, behind the \"\"Dunkin Donuts\"\"\\'Call:  show contact infoMM 37226credit card OK    \"\"GRAND OPENING\"\" ASIAN MASAGE\",,,,,,,,,,,,,,,,,,,'), ((u'5', u'thp'), '\"331 SPAWelcome to best asian massage, Shiatsu and Swedish, Walk on your back, firm or relaxing massage.Great table showerOur place is very clea and very comfortable.Every therapists are licensed and very professional and friendly.You\\'ll be a New Person.Half Hr : $60.00One  Hr:   $80.00Open 9 am to 10 pm, 7days a weekCall:  show contact info*from the Panama city,  beach :   one mile before the I-10 on highway 331 to your left*from the Tallahassee & Mariana :           I-10 Exit 85,   left turn to 331 one mile on your right*from the  Mobil ,Crestview, Pensacola : I-10 Exit 85, right turn to 331 one mile on your right    ASIAN PROFESSIONAL MASSAGE \"\"right off, I-10 on 331 S\"\"\",,,,,,,,,,,,,,,,,,,')]\n",
      "[((u'1', u'thp'), '\"Daziran Massage TherapyCall to make appointment.1314 Westgate Parkway, Suite 4 Dothan, AlabamaWe specialize in Back Walking, Deep Tissue, Sport Massage...etc$60/hour $65/hour deep tissue, we also have 90 Mins and 2 hours session.out call only within Dothan area.Open Monday to Saturday, 10am - 9pm, Sunday 1pm - 8pmshow contact infoAll major credit cards accepted    ***Daziran Massage***334-446-3721\",,,,,,,,,,,,,,,,,,,'), ((u'2', u'thp'), '\"331 SPAWelcome to best asian massage. shiatsu and swedish, walk on your back , firm or relaxing massage.Great table shower.Our place is very clean and very comfortable.Every therapists are licensed and very professional and friendly.You\\'ll be a New person,In callHalf Hr: $60.00 with table showerOne Hr: $80.00 with table showerwe do accept credit cardsopen 9 am to 10 pm , 7days a weekTel:  show contact info*from panama city or beach: one mile before to I-10 on highway 331 to your left* from Tallahassee: Left turn to highway 331 (Exit 85) , we are 1 mile from I-10 on your right side building.*I-10 from alabama(Mobil) or Pesacola Fl - right turn to highway 331 and 1mile to your right.    ASIAN PROFESSIONAL MASSAGE \"\"right off, I-10 on 331 S\"\"\",,,,,,,,,,,,,,,,,,,'), ((u'3', u'thp'), \"Hi I'm Kali and i have the most wonderful relaxing body rubs in town...........pls call or text me for the rates    *******HEAVENLY BODY********,,,,,,,,,,,,,,,,,,,\"), ((u'4', u'thp'), '\"JOUN SPA.................We do Shiatsu, Swedish & Deep-tissue Combination massageYou\\'ll receive best quality massage in town.Half Hr: $60.00One Hr: $80.00 Include Table Showeropen 9am to 10pm We have additional parkings are back of the building and use the back entrance, pleaseOur location is between 23rd st and airport rd, behind the \"\"Dunkin Donuts\"\"\\'Call:  show contact infoMM 37226credit card OK    \"\"GRAND OPENING\"\" ASIAN MASAGE\",,,,,,,,,,,,,,,,,,,'), ((u'5', u'thp'), '\"331 SPAWelcome to best asian massage, Shiatsu and Swedish, Walk on your back, firm or relaxing massage.Great table showerOur place is very clea and very comfortable.Every therapists are licensed and very professional and friendly.You\\'ll be a New Person.Half Hr : $60.00One  Hr:   $80.00Open 9 am to 10 pm, 7days a weekCall:  show contact info*from the Panama city,  beach :   one mile before the I-10 on highway 331 to your left*from the Tallahassee & Mariana :           I-10 Exit 85,   left turn to 331 one mile on your right*from the  Mobil ,Crestview, Pensacola : I-10 Exit 85, right turn to 331 one mile on your right    ASIAN PROFESSIONAL MASSAGE \"\"right off, I-10 on 331 S\"\"\",,,,,,,,,,,,,,,,,,,')]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[((u'1', u'thp'), [u'``', u'Daziran', u'Massage', u'TherapyCall', u'to', u'make', u'appointment.1314', u'Westgate', u'Parkway', u',', u'Suite', u'4', u'Dothan', u',', u'AlabamaWe', u'specialize', u'in', u'Back', u'Walking', u',', u'Deep', u'Tissue', u',', u'Sport', u'Massage', u'...', u'etc', u'$', u'60/hour', u'$', u'65/hour', u'deep', u'tissue', u',', u'we', u'also', u'have', u'90', u'Mins', u'and', u'2', u'hours', u'session.out', u'call', u'only', u'within', u'Dothan', u'area.Open', u'Monday', u'to', u'Saturday', u',', u'10am', u'-', u'9pm', u',', u'Sunday', u'1pm', u'-', u'8pmshow', u'contact', u'infoAll', u'major', u'credit', u'cards', u'accepted', u'***Daziran', u'Massage***334-446-3721', u\"''\", u',', u',', u',', u',', u',', u',', u',', u',', u',', u',', u',', u',', u',', u',', u',', u',', u',', u',', u',']), ((u'2', u'thp'), [u'``', u'331', u'SPAWelcome', u'to', u'best', u'asian', u'massage', u'.', u'shiatsu', u'and', u'swedish', u',', u'walk', u'on', u'your', u'back', u',', u'firm', u'or', u'relaxing', u'massage.Great', u'table', u'shower.Our', u'place', u'is', u'very', u'clean', u'and', u'very', u'comfortable.Every', u'therapists', u'are', u'licensed', u'and', u'very', u'professional', u'and', u'friendly.You', u\"'ll\", u'be', u'a', u'New', u'person', u',', u'In', u'callHalf', u'Hr', u':', u'$', u'60.00', u'with', u'table', u'showerOne', u'Hr', u':', u'$', u'80.00', u'with', u'table', u'showerwe', u'do', u'accept', u'credit', u'cardsopen', u'9', u'am', u'to', u'10', u'pm', u',', u'7days', u'a', u'weekTel', u':', u'show', u'contact', u'info*from', u'panama', u'city', u'or', u'beach', u':', u'one', u'mile', u'before', u'to', u'I-10', u'on', u'highway', u'331', u'to', u'your', u'left*', u'from', u'Tallahassee', u':', u'Left', u'turn', u'to', u'highway', u'331', u'(', u'Exit', u'85', u')', u',', u'we', u'are', u'1', u'mile', u'from', u'I-10', u'on', u'your', u'right', u'side', u'building', u'.', u'*I-10', u'from', u'alabama', u'(', u'Mobil', u')', u'or', u'Pesacola', u'Fl', u'-', u'right', u'turn', u'to', u'highway', u'331', u'and', u'1mile', u'to', u'your', u'right', u'.', u'ASIAN', u'PROFESSIONAL', u'MASSAGE', u'``', u\"''\", u'right', u'off', u',', u'I-10', u'on', u'331', u'S', u\"''\", u\"''\", u\"''\", u',', u',', u',', u',', u',', u',', u',', u',', u',', u',', u',', u',', u',', u',', u',', u',', u',', u',', u',']), ((u'3', u'thp'), [u'Hi', u'I', u\"'m\", u'Kali', u'and', u'i', u'have', u'the', u'most', u'wonderful', u'relaxing', u'body', u'rubs', u'in', u'town', u'...', u'...', u'...', u'..pls', u'call', u'or', u'text', u'me', u'for', u'the', u'rates', u'*******HEAVENLY', u'BODY********', u',', u',', u',', u',', u',', u',', u',', u',', u',', u',', u',', u',', u',', u',', u',', u',', u',', u',', u',']), ((u'4', u'thp'), [u'``', u'JOUN', u'SPA', u'...', u'...', u'...', u'...', u'...', u'..We', u'do', u'Shiatsu', u',', u'Swedish', u'&', u'Deep-tissue', u'Combination', u'massageYou', u\"'ll\", u'receive', u'best', u'quality', u'massage', u'in', u'town.Half', u'Hr', u':', u'$', u'60.00One', u'Hr', u':', u'$', u'80.00', u'Include', u'Table', u'Showeropen', u'9am', u'to', u'10pm', u'We', u'have', u'additional', u'parkings', u'are', u'back', u'of', u'the', u'building', u'and', u'use', u'the', u'back', u'entrance', u',', u'pleaseOur', u'location', u'is', u'between', u'23rd', u'st', u'and', u'airport', u'rd', u',', u'behind', u'the', u'``', u\"''\", u'Dunkin', u'Donuts', u\"''\", u\"''\", u\"'Call\", u':', u'show', u'contact', u'infoMM', u'37226credit', u'card', u'OK', u'``', u\"''\", u'GRAND', u'OPENING', u\"''\", u\"''\", u'ASIAN', u'MASAGE', u\"''\", u',', u',', u',', u',', u',', u',', u',', u',', u',', u',', u',', u',', u',', u',', u',', u',', u',', u',', u',']), ((u'5', u'thp'), [u'``', u'331', u'SPAWelcome', u'to', u'best', u'asian', u'massage', u',', u'Shiatsu', u'and', u'Swedish', u',', u'Walk', u'on', u'your', u'back', u',', u'firm', u'or', u'relaxing', u'massage.Great', u'table', u'showerOur', u'place', u'is', u'very', u'clea', u'and', u'very', u'comfortable.Every', u'therapists', u'are', u'licensed', u'and', u'very', u'professional', u'and', u'friendly.You', u\"'ll\", u'be', u'a', u'New', u'Person.Half', u'Hr', u':', u'$', u'60.00One', u'Hr', u':', u'$', u'80.00Open', u'9', u'am', u'to', u'10', u'pm', u',', u'7days', u'a', u'weekCall', u':', u'show', u'contact', u'info*from', u'the', u'Panama', u'city', u',', u'beach', u':', u'one', u'mile', u'before', u'the', u'I-10', u'on', u'highway', u'331', u'to', u'your', u'left*from', u'the', u'Tallahassee', u'&', u'Mariana', u':', u'I-10', u'Exit', u'85', u',', u'left', u'turn', u'to', u'331', u'one', u'mile', u'on', u'your', u'right*from', u'the', u'Mobil', u',', u'Crestview', u',', u'Pensacola', u':', u'I-10', u'Exit', u'85', u',', u'right', u'turn', u'to', u'331', u'one', u'mile', u'on', u'your', u'right', u'ASIAN', u'PROFESSIONAL', u'MASSAGE', u'``', u\"''\", u'right', u'off', u',', u'I-10', u'on', u'331', u'S', u\"''\", u\"''\", u\"''\", u',', u',', u',', u',', u',', u',', u',', u',', u',', u',', u',', u',', u',', u',', u',', u',', u',', u',', u','])]\n",
      "[((u'1', u'thp'), ['``', 'daziran', 'massage', 'therapycall', 'to', 'make', 'appointment.1314', 'westgate', 'parkway', ',', 'suite', '4', 'dothan', ',', 'alabamawe', 'specialize', 'in', 'back', 'walking', ',', 'deep', 'tissue', ',', 'sport', 'massage', '...', 'etc', '$', '60/hour', '$', '65/hour', 'deep', 'tissue', ',', 'we', 'also', 'have', '90', 'mins', 'and', '2', 'hours', 'session.out', 'call', 'only', 'within', 'dothan', 'area.open', 'monday', 'to', 'saturday', ',', '10am', '-', '9pm', ',', 'sunday', '1pm', '-', '8pmshow', 'contact', 'infoall', 'major', 'credit', 'cards', 'accepted', '***daziran', 'massage***334-446-3721', \"''\", ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',']), ((u'2', u'thp'), ['``', '331', 'spawelcome', 'to', 'best', 'asian', 'massage', '.', 'shiatsu', 'and', 'swedish', ',', 'walk', 'on', 'your', 'back', ',', 'firm', 'or', 'relaxing', 'massage.great', 'table', 'shower.our', 'place', 'is', 'very', 'clean', 'and', 'very', 'comfortable.every', 'therapists', 'are', 'licensed', 'and', 'very', 'professional', 'and', 'friendly.you', \"'ll\", 'be', 'a', 'new', 'person', ',', 'in', 'callhalf', 'hr', ':', '$', '60.00', 'with', 'table', 'showerone', 'hr', ':', '$', '80.00', 'with', 'table', 'showerwe', 'do', 'accept', 'credit', 'cardsopen', '9', 'am', 'to', '10', 'pm', ',', '7days', 'a', 'weektel', ':', 'show', 'contact', 'info*from', 'panama', 'city', 'or', 'beach', ':', 'one', 'mile', 'before', 'to', 'i-10', 'on', 'highway', '331', 'to', 'your', 'left*', 'from', 'tallahassee', ':', 'left', 'turn', 'to', 'highway', '331', '(', 'exit', '85', ')', ',', 'we', 'are', '1', 'mile', 'from', 'i-10', 'on', 'your', 'right', 'side', 'building', '.', '*i-10', 'from', 'alabama', '(', 'mobil', ')', 'or', 'pesacola', 'fl', '-', 'right', 'turn', 'to', 'highway', '331', 'and', '1mile', 'to', 'your', 'right', '.', 'asian', 'professional', 'massage', '``', \"''\", 'right', 'off', ',', 'i-10', 'on', '331', 's', \"''\", \"''\", \"''\", ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',']), ((u'3', u'thp'), ['hi', 'i', \"'m\", 'kali', 'and', 'i', 'have', 'the', 'most', 'wonderful', 'relaxing', 'body', 'rubs', 'in', 'town', '...', '...', '...', '..pls', 'call', 'or', 'text', 'me', 'for', 'the', 'rates', '*******heavenly', 'body********', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',']), ((u'4', u'thp'), ['``', 'joun', 'spa', '...', '...', '...', '...', '...', '..we', 'do', 'shiatsu', ',', 'swedish', '&', 'deep-tissue', 'combination', 'massageyou', \"'ll\", 'receive', 'best', 'quality', 'massage', 'in', 'town.half', 'hr', ':', '$', '60.00one', 'hr', ':', '$', '80.00', 'include', 'table', 'showeropen', '9am', 'to', '10pm', 'we', 'have', 'additional', 'parkings', 'are', 'back', 'of', 'the', 'building', 'and', 'use', 'the', 'back', 'entrance', ',', 'pleaseour', 'location', 'is', 'between', '23rd', 'st', 'and', 'airport', 'rd', ',', 'behind', 'the', '``', \"''\", 'dunkin', 'donuts', \"''\", \"''\", \"'call\", ':', 'show', 'contact', 'infomm', '37226credit', 'card', 'ok', '``', \"''\", 'grand', 'opening', \"''\", \"''\", 'asian', 'masage', \"''\", ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',']), ((u'5', u'thp'), ['``', '331', 'spawelcome', 'to', 'best', 'asian', 'massage', ',', 'shiatsu', 'and', 'swedish', ',', 'walk', 'on', 'your', 'back', ',', 'firm', 'or', 'relaxing', 'massage.great', 'table', 'showerour', 'place', 'is', 'very', 'clea', 'and', 'very', 'comfortable.every', 'therapists', 'are', 'licensed', 'and', 'very', 'professional', 'and', 'friendly.you', \"'ll\", 'be', 'a', 'new', 'person.half', 'hr', ':', '$', '60.00one', 'hr', ':', '$', '80.00open', '9', 'am', 'to', '10', 'pm', ',', '7days', 'a', 'weekcall', ':', 'show', 'contact', 'info*from', 'the', 'panama', 'city', ',', 'beach', ':', 'one', 'mile', 'before', 'the', 'i-10', 'on', 'highway', '331', 'to', 'your', 'left*from', 'the', 'tallahassee', '&', 'mariana', ':', 'i-10', 'exit', '85', ',', 'left', 'turn', 'to', '331', 'one', 'mile', 'on', 'your', 'right*from', 'the', 'mobil', ',', 'crestview', ',', 'pensacola', ':', 'i-10', 'exit', '85', ',', 'right', 'turn', 'to', '331', 'one', 'mile', 'on', 'your', 'right', 'asian', 'professional', 'massage', '``', \"''\", 'right', 'off', ',', 'i-10', 'on', '331', 's', \"''\", \"''\", \"''\", ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ','])]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[((u'1', u'thp'), ['', 'daziran', 'massage', 'therapycall', 'to', 'make', 'appointment1314', 'westgate', 'parkway', '', 'suite', '4', 'dothan', '', 'alabamawe', 'specialize', 'in', 'back', 'walking', '', 'deep', 'tissue', '', 'sport', 'massage', '', 'etc', '', '60hour', '', '65hour', 'deep', 'tissue', '', 'we', 'also', 'have', '90', 'mins', 'and', '2', 'hours', 'sessionout', 'call', 'only', 'within', 'dothan', 'areaopen', 'monday', 'to', 'saturday', '', '10am', '', '9pm', '', 'sunday', '1pm', '', '8pmshow', 'contact', 'infoall', 'major', 'credit', 'cards', 'accepted', 'daziran', 'massage3344463721', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']), ((u'2', u'thp'), ['', '331', 'spawelcome', 'to', 'best', 'asian', 'massage', '', 'shiatsu', 'and', 'swedish', '', 'walk', 'on', 'your', 'back', '', 'firm', 'or', 'relaxing', 'massagegreat', 'table', 'showerour', 'place', 'is', 'very', 'clean', 'and', 'very', 'comfortableevery', 'therapists', 'are', 'licensed', 'and', 'very', 'professional', 'and', 'friendlyyou', 'll', 'be', 'a', 'new', 'person', '', 'in', 'callhalf', 'hr', '', '', '6000', 'with', 'table', 'showerone', 'hr', '', '', '8000', 'with', 'table', 'showerwe', 'do', 'accept', 'credit', 'cardsopen', '9', 'am', 'to', '10', 'pm', '', '7days', 'a', 'weektel', '', 'show', 'contact', 'infofrom', 'panama', 'city', 'or', 'beach', '', 'one', 'mile', 'before', 'to', 'i10', 'on', 'highway', '331', 'to', 'your', 'left', 'from', 'tallahassee', '', 'left', 'turn', 'to', 'highway', '331', '', 'exit', '85', '', '', 'we', 'are', '1', 'mile', 'from', 'i10', 'on', 'your', 'right', 'side', 'building', '', 'i10', 'from', 'alabama', '', 'mobil', '', 'or', 'pesacola', 'fl', '', 'right', 'turn', 'to', 'highway', '331', 'and', '1mile', 'to', 'your', 'right', '', 'asian', 'professional', 'massage', '', '', 'right', 'off', '', 'i10', 'on', '331', 's', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']), ((u'3', u'thp'), ['hi', 'i', 'm', 'kali', 'and', 'i', 'have', 'the', 'most', 'wonderful', 'relaxing', 'body', 'rubs', 'in', 'town', '', '', '', 'pls', 'call', 'or', 'text', 'me', 'for', 'the', 'rates', 'heavenly', 'body', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']), ((u'4', u'thp'), ['', 'joun', 'spa', '', '', '', '', '', 'we', 'do', 'shiatsu', '', 'swedish', '', 'deeptissue', 'combination', 'massageyou', 'll', 'receive', 'best', 'quality', 'massage', 'in', 'townhalf', 'hr', '', '', '6000one', 'hr', '', '', '8000', 'include', 'table', 'showeropen', '9am', 'to', '10pm', 'we', 'have', 'additional', 'parkings', 'are', 'back', 'of', 'the', 'building', 'and', 'use', 'the', 'back', 'entrance', '', 'pleaseour', 'location', 'is', 'between', '23rd', 'st', 'and', 'airport', 'rd', '', 'behind', 'the', '', '', 'dunkin', 'donuts', '', '', 'call', '', 'show', 'contact', 'infomm', '37226credit', 'card', 'ok', '', '', 'grand', 'opening', '', '', 'asian', 'masage', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']), ((u'5', u'thp'), ['', '331', 'spawelcome', 'to', 'best', 'asian', 'massage', '', 'shiatsu', 'and', 'swedish', '', 'walk', 'on', 'your', 'back', '', 'firm', 'or', 'relaxing', 'massagegreat', 'table', 'showerour', 'place', 'is', 'very', 'clea', 'and', 'very', 'comfortableevery', 'therapists', 'are', 'licensed', 'and', 'very', 'professional', 'and', 'friendlyyou', 'll', 'be', 'a', 'new', 'personhalf', 'hr', '', '', '6000one', 'hr', '', '', '8000open', '9', 'am', 'to', '10', 'pm', '', '7days', 'a', 'weekcall', '', 'show', 'contact', 'infofrom', 'the', 'panama', 'city', '', 'beach', '', 'one', 'mile', 'before', 'the', 'i10', 'on', 'highway', '331', 'to', 'your', 'leftfrom', 'the', 'tallahassee', '', 'mariana', '', 'i10', 'exit', '85', '', 'left', 'turn', 'to', '331', 'one', 'mile', 'on', 'your', 'rightfrom', 'the', 'mobil', '', 'crestview', '', 'pensacola', '', 'i10', 'exit', '85', '', 'right', 'turn', 'to', '331', 'one', 'mile', 'on', 'your', 'right', 'asian', 'professional', 'massage', '', '', 'right', 'off', '', 'i10', 'on', '331', 's', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''])]\n",
      "[((u'1', u'thp'), ['daziran', 'massage', 'therapycall', 'to', 'make', 'westgate', 'parkway', 'suite', 'dothan', 'alabamawe', 'specialize', 'in', 'back', 'walking', 'deep', 'tissue', 'sport', 'massage', 'etc', 'deep', 'tissue', 'we', 'also', 'have', 'mins', 'and', 'hours', 'sessionout', 'call', 'only', 'within', 'dothan', 'areaopen', 'monday', 'to', 'saturday', 'sunday', 'contact', 'infoall', 'major', 'credit', 'cards', 'accepted', 'daziran']), ((u'2', u'thp'), ['spawelcome', 'to', 'best', 'asian', 'massage', 'shiatsu', 'and', 'swedish', 'walk', 'on', 'your', 'back', 'firm', 'or', 'relaxing', 'massagegreat', 'table', 'showerour', 'place', 'is', 'very', 'clean', 'and', 'very', 'comfortableevery', 'therapists', 'are', 'licensed', 'and', 'very', 'professional', 'and', 'friendlyyou', 'll', 'be', 'a', 'new', 'person', 'in', 'callhalf', 'hr', 'with', 'table', 'showerone', 'hr', 'with', 'table', 'showerwe', 'do', 'accept', 'credit', 'cardsopen', 'am', 'to', 'pm', 'a', 'weektel', 'show', 'contact', 'infofrom', 'panama', 'city', 'or', 'beach', 'one', 'mile', 'before', 'to', 'on', 'highway', 'to', 'your', 'left', 'from', 'tallahassee', 'left', 'turn', 'to', 'highway', 'exit', 'we', 'are', 'mile', 'from', 'on', 'your', 'right', 'side', 'building', 'from', 'alabama', 'mobil', 'or', 'pesacola', 'fl', 'right', 'turn', 'to', 'highway', 'and', 'to', 'your', 'right', 'asian', 'professional', 'massage', 'right', 'off', 'on', 's']), ((u'3', u'thp'), ['hi', 'i', 'm', 'kali', 'and', 'i', 'have', 'the', 'most', 'wonderful', 'relaxing', 'body', 'rubs', 'in', 'town', 'pls', 'call', 'or', 'text', 'me', 'for', 'the', 'rates', 'heavenly', 'body']), ((u'4', u'thp'), ['joun', 'spa', 'we', 'do', 'shiatsu', 'swedish', 'deeptissue', 'combination', 'massageyou', 'll', 'receive', 'best', 'quality', 'massage', 'in', 'townhalf', 'hr', 'hr', 'include', 'table', 'showeropen', 'to', 'we', 'have', 'additional', 'parkings', 'are', 'back', 'of', 'the', 'building', 'and', 'use', 'the', 'back', 'entrance', 'pleaseour', 'location', 'is', 'between', 'st', 'and', 'airport', 'rd', 'behind', 'the', 'dunkin', 'donuts', 'call', 'show', 'contact', 'infomm', 'card', 'ok', 'grand', 'opening', 'asian', 'masage']), ((u'5', u'thp'), ['spawelcome', 'to', 'best', 'asian', 'massage', 'shiatsu', 'and', 'swedish', 'walk', 'on', 'your', 'back', 'firm', 'or', 'relaxing', 'massagegreat', 'table', 'showerour', 'place', 'is', 'very', 'clea', 'and', 'very', 'comfortableevery', 'therapists', 'are', 'licensed', 'and', 'very', 'professional', 'and', 'friendlyyou', 'll', 'be', 'a', 'new', 'personhalf', 'hr', 'hr', 'am', 'to', 'pm', 'a', 'weekcall', 'show', 'contact', 'infofrom', 'the', 'panama', 'city', 'beach', 'one', 'mile', 'before', 'the', 'on', 'highway', 'to', 'your', 'leftfrom', 'the', 'tallahassee', 'mariana', 'exit', 'left', 'turn', 'to', 'one', 'mile', 'on', 'your', 'rightfrom', 'the', 'mobil', 'crestview', 'pensacola', 'exit', 'right', 'turn', 'to', 'one', 'mile', 'on', 'your', 'right', 'asian', 'professional', 'massage', 'right', 'off', 'on', 's'])]\n",
      "[((u'1', u'thp'), ['daziran', 'massage', 'therapycall', 'make', 'westgate', 'parkway', 'suite', 'dothan', 'alabamawe', 'specialize', 'back', 'walking', 'deep', 'tissue', 'sport', 'massage', 'etc', 'deep', 'tissue', 'also', 'mins', 'hours', 'sessionout', 'call', 'within', 'dothan', 'areaopen', 'monday', 'saturday', 'sunday', 'contact', 'infoall', 'major', 'credit', 'cards', 'accepted', 'daziran']), ((u'2', u'thp'), ['spawelcome', 'best', 'asian', 'massage', 'shiatsu', 'swedish', 'walk', 'back', 'firm', 'relaxing', 'massagegreat', 'table', 'showerour', 'place', 'clean', 'comfortableevery', 'therapists', 'licensed', 'professional', 'friendlyyou', 'new', 'person', 'callhalf', 'hr', 'table', 'showerone', 'hr', 'table', 'showerwe', 'accept', 'credit', 'cardsopen', 'pm', 'weektel', 'show', 'contact', 'infofrom', 'panama', 'city', 'beach', 'one', 'mile', 'highway', 'left', 'tallahassee', 'left', 'turn', 'highway', 'exit', 'mile', 'right', 'side', 'building', 'alabama', 'mobil', 'pesacola', 'fl', 'right', 'turn', 'highway', 'right', 'asian', 'professional', 'massage', 'right']), ((u'3', u'thp'), ['hi', 'kali', 'wonderful', 'relaxing', 'body', 'rubs', 'town', 'pls', 'call', 'text', 'rates', 'heavenly', 'body']), ((u'4', u'thp'), ['joun', 'spa', 'shiatsu', 'swedish', 'deeptissue', 'combination', 'massageyou', 'receive', 'best', 'quality', 'massage', 'townhalf', 'hr', 'hr', 'include', 'table', 'showeropen', 'additional', 'parkings', 'back', 'building', 'use', 'back', 'entrance', 'pleaseour', 'location', 'st', 'airport', 'rd', 'behind', 'dunkin', 'donuts', 'call', 'show', 'contact', 'infomm', 'card', 'ok', 'grand', 'opening', 'asian', 'masage']), ((u'5', u'thp'), ['spawelcome', 'best', 'asian', 'massage', 'shiatsu', 'swedish', 'walk', 'back', 'firm', 'relaxing', 'massagegreat', 'table', 'showerour', 'place', 'clea', 'comfortableevery', 'therapists', 'licensed', 'professional', 'friendlyyou', 'new', 'personhalf', 'hr', 'hr', 'pm', 'weekcall', 'show', 'contact', 'infofrom', 'panama', 'city', 'beach', 'one', 'mile', 'highway', 'leftfrom', 'tallahassee', 'mariana', 'exit', 'left', 'turn', 'one', 'mile', 'rightfrom', 'mobil', 'crestview', 'pensacola', 'exit', 'right', 'turn', 'one', 'mile', 'right', 'asian', 'professional', 'massage', 'right'])]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[((u'1', u'thp'), ['daziran', u'massag', u'therapycal', 'make', u'westgat', 'parkway', u'suit', 'dothan', u'alabamaw', u'special', 'back', u'walk', 'deep', u'tissu', 'sport', u'massag', 'etc', 'deep', u'tissu', 'also', u'min', u'hour', 'sessionout', 'call', 'within', 'dothan', 'areaopen', 'monday', 'saturday', 'sunday', 'contact', u'infoal', 'major', 'credit', u'card', u'accept', 'daziran']), ((u'2', u'thp'), [u'spawelcom', 'best', 'asian', u'massag', 'shiatsu', 'swedish', 'walk', 'back', 'firm', u'relax', 'massagegreat', u'tabl', 'showerour', 'place', 'clean', u'comfortableeveri', u'therapist', u'licens', u'profession', u'friendlyy', 'new', 'person', 'callhalf', 'hr', u'tabl', u'showeron', 'hr', u'tabl', u'showerw', 'accept', 'credit', 'cardsopen', 'pm', 'weektel', 'show', 'contact', 'infofrom', 'panama', u'citi', 'beach', 'one', 'mile', 'highway', 'left', u'tallahasse', 'left', 'turn', 'highway', 'exit', 'mile', 'right', 'side', u'build', 'alabama', 'mobil', 'pesacola', 'fl', 'right', 'turn', 'highway', 'right', 'asian', u'profession', u'massag', 'right']), ((u'3', u'thp'), ['hi', 'kali', u'wonder', u'relax', u'bodi', u'rub', 'town', u'pl', 'call', 'text', u'rate', u'heavenli', u'bodi']), ((u'4', u'thp'), ['joun', 'spa', 'shiatsu', 'swedish', u'deeptissu', u'combin', u'massagey', u'receiv', 'best', u'qualiti', u'massag', 'townhalf', 'hr', 'hr', u'includ', u'tabl', 'showeropen', u'addit', u'park', 'back', u'build', 'use', 'back', u'entranc', 'pleaseour', u'locat', 'st', 'airport', 'rd', 'behind', 'dunkin', u'donut', 'call', 'show', 'contact', 'infomm', 'card', 'ok', 'grand', u'open', 'asian', u'masag']), ((u'5', u'thp'), [u'spawelcom', 'best', 'asian', u'massag', 'shiatsu', 'swedish', 'walk', 'back', 'firm', u'relax', 'massagegreat', u'tabl', 'showerour', 'place', 'clea', u'comfortableeveri', u'therapist', u'licens', u'profession', u'friendlyy', 'new', 'personhalf', 'hr', 'hr', 'pm', u'weekcal', 'show', 'contact', 'infofrom', 'panama', u'citi', 'beach', 'one', 'mile', 'highway', 'leftfrom', u'tallahasse', 'mariana', 'exit', 'left', 'turn', 'one', 'mile', 'rightfrom', 'mobil', 'crestview', 'pensacola', 'exit', 'right', 'turn', 'one', 'mile', 'right', 'asian', u'profession', u'massag', 'right'])]\n",
      "['daziran massag therapycal make westgat parkway suit dothan alabamaw special back walk deep tissu sport massag etc deep tissu also min hour sessionout call within dothan areaopen monday saturday sunday contact infoal major credit card accept daziran', 'spawelcom best asian massag shiatsu swedish walk back firm relax massagegreat tabl showerour place clean comfortableeveri therapist licens profession friendlyy new person callhalf hr tabl showeron hr tabl showerw accept credit cardsopen pm weektel show contact infofrom panama citi beach one mile highway left tallahasse left turn highway exit mile right side build alabama mobil pesacola fl right turn highway right asian profession massag right', 'hi kali wonder relax bodi rub town pl call text rate heavenli bodi', 'joun spa shiatsu swedish deeptissu combin massagey receiv best qualiti massag townhalf hr hr includ tabl showeropen addit park back build use back entranc pleaseour locat st airport rd behind dunkin donut call show contact infomm card ok grand open asian masag', 'spawelcom best asian massag shiatsu swedish walk back firm relax massagegreat tabl showerour place clea comfortableeveri therapist licens profession friendlyy new personhalf hr hr pm weekcal show contact infofrom panama citi beach one mile highway leftfrom tallahasse mariana exit left turn one mile rightfrom mobil crestview pensacola exit right turn one mile right asian profession massag right']\n",
      "ready for clustering\n"
     ]
    }
   ],
   "source": [
    "#---------------process ads to be use\n",
    "verbose = True\n",
    "# convert to string from utf-8 and split on first two commas\n",
    "# encoded_rdd = raw_ads.map(lambda x: str(x.encode('utf-8'))).map(lambda x: x.split(\",\",2))\n",
    "encoded_rdd = raw_ads.map(lambda x: x.split(\",\",2))\n",
    "# encoded_rdd.take(3)\n",
    "if verbose: print(encoded_rdd.collect()[:5])\n",
    "\n",
    "# remove blank ads and create kv pairs\n",
    "# [((id, cl_listing), ad), ...]\n",
    "kv_rdd = encoded_rdd.filter(lambda x: x[2].strip() != \"\").map(lambda x: ((x[0], x[1]), x[2]))\n",
    "if verbose: print(kv_rdd.collect()[:5])\n",
    "    \n",
    "# remove none\n",
    "# [((id, cl_listing), ad), ...]\n",
    "utf_ads = kv_rdd.map(lambda (k, text): (k,''.join(str(x) for x in text.encode('utf-8') if ord(x) < 128)))#.map(lambda x: x[1].decode('utf-8'))\n",
    "if verbose: print(utf_ads.collect()[:5])\n",
    "    \n",
    "filtered_ads = utf_ads.filter(lambda (k,v): v.strip() != \"\")\n",
    "if verbose: print(filtered_ads.collect()[:5])\n",
    "\n",
    "# tokenize words \n",
    "#[((id, cl_listing), [token_I, ....]), ...]\n",
    "# tokenized_rdd = filtered_ads.map(lambda (k,v): (k, word_tokenize(v)))\n",
    "tokenized_rdd = kv_rdd.map(lambda (k,v): (k, word_tokenize(v)))\n",
    "if verbose: print(tokenized_rdd.collect()[:5])\n",
    "\n",
    "# convert text to lowercase\n",
    "lower_case_rdd = tokenized_rdd.map(lambda (k,v): (k, [token.encode('utf-8').lower() for token in v]))\n",
    "if verbose: print(lower_case_rdd.collect()[:5])\n",
    "    \n",
    "# remove punctuation from words\n",
    "stripped_punct_rdd = lower_case_rdd.map(lambda (k,text): (k, [s.translate(None, string.punctuation) for s in text]))\n",
    "if verbose: print(stripped_punct_rdd.collect()[:5])\n",
    "    \n",
    "# stripped_punct_rdd = stripped_punct_rdd.map(lambda (k,text): (k, [s.translate(None, '') for s in text]\n",
    "# remove non text tokens\n",
    "alpha_rdd = stripped_punct_rdd.map(lambda (k,text): (k, [word for word in text if word.isalpha()]))\n",
    "if verbose: print(alpha_rdd.collect()[:5])\n",
    "    \n",
    "# remove stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stopped_rdd = alpha_rdd.map(lambda (k,text): (k, [word for word in text if word not in stop_words]))\n",
    "if verbose: print(stopped_rdd.collect()[:5])\n",
    "    \n",
    "porter = PorterStemmer()\n",
    "stemmed_rdd = stopped_rdd.map(lambda (k,text): (k, [porter.stem(word) for word in text]))\n",
    "if verbose: print(stemmed_rdd.collect()[:5])\n",
    "    \n",
    "# text has been processed for clustering convert back to strings\n",
    "processed_rdd = stemmed_rdd.map(lambda (k,txt): (k, \" \".join(w for w in txt))).map(lambda (k,txt): str(txt.encode('utf-8')))\n",
    "processed_rdd = processed_rdd.filter(lambda txt: txt != \" \")\n",
    "if verbose: print(processed_rdd.collect()[:5])\n",
    "\n",
    "processed_rdd.persist()\n",
    "# print(processed_rdd.take(4))\n",
    "\n",
    "print(\"ready for clustering\")\n",
    "#----------------ready for clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o301.fitIDF.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 15.0 failed 1 times, most recent failure: Lost task 1.0 in stage 15.0 (TID 29, localhost, executor driver): TaskResultLost (result lost from block manager)\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2022)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2119)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1026)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\r\n\tat org.apache.spark.rdd.RDD.reduce(RDD.scala:1008)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1.apply(RDD.scala:1151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\r\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1128)\r\n\tat org.apache.spark.mllib.feature.IDF.fit(IDF.scala:54)\r\n\tat org.apache.spark.mllib.feature.IDF.fit(IDF.scala:67)\r\n\tat org.apache.spark.mllib.api.python.PythonMLLibAPI.fitIDF(PythonMLLibAPI.scala:672)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:280)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-54da214173e0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;31m# First to compute the IDF vector and second to scale the term frequencies by IDF.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0midf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mIDF\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[0mtfidf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0midf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\opt\\spark\\spark-2.2.0-bin-hadoop2.7\\python\\pyspark\\mllib\\feature.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    571\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    572\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"dataset should be an RDD of term frequency vectors\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 573\u001b[1;33m         \u001b[0mjmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcallMLlibFunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"fitIDF\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mminDocFreq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_convert_to_vector\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    574\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mIDFModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    575\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\opt\\spark\\spark-2.2.0-bin-hadoop2.7\\python\\pyspark\\mllib\\common.pyc\u001b[0m in \u001b[0;36mcallMLlibFunc\u001b[1;34m(name, *args)\u001b[0m\n\u001b[0;32m    128\u001b[0m     \u001b[0msc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m     \u001b[0mapi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonMLLibAPI\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mcallJavaFunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mapi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    131\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\opt\\spark\\spark-2.2.0-bin-hadoop2.7\\python\\pyspark\\mllib\\common.pyc\u001b[0m in \u001b[0;36mcallJavaFunc\u001b[1;34m(sc, func, *args)\u001b[0m\n\u001b[0;32m    121\u001b[0m     \u001b[1;34m\"\"\" Call Java Function \"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m     \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0m_py2java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[1;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 123\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_java2py\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    124\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\opt\\spark\\spark-2.2.0-bin-hadoop2.7\\python\\lib\\py4j-0.10.4-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1133\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1135\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\opt\\spark\\spark-2.2.0-bin-hadoop2.7\\python\\pyspark\\sql\\utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\opt\\spark\\spark-2.2.0-bin-hadoop2.7\\python\\lib\\py4j-0.10.4-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    318\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 319\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    320\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o301.fitIDF.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 15.0 failed 1 times, most recent failure: Lost task 1.0 in stage 15.0 (TID 29, localhost, executor driver): TaskResultLost (result lost from block manager)\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2022)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2119)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1026)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\r\n\tat org.apache.spark.rdd.RDD.reduce(RDD.scala:1008)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1.apply(RDD.scala:1151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\r\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1128)\r\n\tat org.apache.spark.mllib.feature.IDF.fit(IDF.scala:54)\r\n\tat org.apache.spark.mllib.feature.IDF.fit(IDF.scala:67)\r\n\tat org.apache.spark.mllib.api.python.PythonMLLibAPI.fitIDF(PythonMLLibAPI.scala:672)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:280)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n"
     ]
    }
   ],
   "source": [
    "#----------------Test clustering\n",
    "svd_n = 15\n",
    "k = 3\n",
    "# model_name = \"bimeans\"\n",
    "model_name = \"kmeans\"\n",
    "\n",
    "# run tf calculator\n",
    "hashingTF = HashingTF()\n",
    "tf = hashingTF.transform(processed_rdd)\n",
    "\n",
    "# While applying HashingTF only needs a single pass to the data, applying IDF needs two passes:\n",
    "# First to compute the IDF vector and second to scale the term frequencies by IDF.\n",
    "tf.cache()\n",
    "idf = IDF().fit(tf)\n",
    "tfidf = idf.transform(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf_rdd = idfIgnore.transform(tf)\n",
    "matrix_rdd = RowMatrix(tfidf_rdd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svd_i = matrix_rdd.computeSVD(svd_n, computeU=True)\n",
    "rdd = svd_i.U.rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if model_name == \"kmeans\":\n",
    "    clust_model = KMeans.train(rdd, k)\n",
    "else:\n",
    "    clust_model = BisectingKMeans.train(rdd, k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cost = clust_model.computeCost(rdd)\n",
    "save_cluster_metrics(model_name, cost, svd_n, k=k)\n",
    "predictions_rdd = rdd.map(lambda x: (x, clust_model.predict(x)))\n",
    "kmeans_predictions_fn=\"predictions_{}_topics{}_k{}\".format(model_name, svd_n, k)\n",
    "# save_cluster_predictions(np.array(predictions_rdd.collect()), model=\"km\", fn=kmeans_predictions_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svd_n = 2\n",
    "k = 3\n",
    "# model_name = \"bimeans\"\n",
    "model_name = \"kmeans\"\n",
    "\n",
    "hashingTF = HashingTF()\n",
    "tf = hashingTF.transform(processed_rdd)\n",
    "idf = IDF().fit(tf)\n",
    "tfidf = idf.transform(tf)\n",
    "idfIgnore = IDF(minDocFreq=2).fit(tf)\n",
    "tfidf_rdd = idfIgnore.transform(tf)\n",
    "matrix_rdd = RowMatrix(tfidf_rdd)\n",
    "svd_i = matrix_rdd.computeSVD(svd_n, computeU=True)\n",
    "rdd = svd_i.U.rows\n",
    "if model_name == \"kmeans\":\n",
    "    clust_model = KMeans.train(rdd, k)\n",
    "else:\n",
    "    clust_model = BisectingKMeans.train(rdd, k)\n",
    "cost = clust_model.computeCost(rdd)\n",
    "save_cluster_metrics(model_name, cost, svd_n, k=k)\n",
    "predictions_rdd = rdd.map(lambda x: (x, clust_model.predict(x)))\n",
    "kmeans_predictions_fn=\"predictions_{}_topics{}_k{}.npy\".format(model_name, svd_n, k)\n",
    "save_cluster_predictions(np.array(predictions_rdd.collect()), model=\"km\", fn=kmeans_predictions_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# [ad0, ad1, ..]\n",
    "# processed = raw_ads.filter(lambda line: line !=header).map(lambda x: (x[0], x[1]))\n",
    "# ads_rdd = raw_ads.map(lambda x: x.split(\",\", 2)).map(lambda x: ((x[0], x[1]),x[2].encode('utf-8', 'ignore')))\n",
    "# ads_rdd.take(3)\n",
    "# processed_rdd = ads_rdd.map(lambda x: x.split(\",\", 2))#.map(lambda x: ((x[0], x[1]), x[2]))\n",
    "# processed_rdd = ads_rdd.map(lambda x: x.split(\",\", 1)).map(lambda x: (int(x[0]), x[1]))\n",
    "# processed_rdd = ads_rdd.map(lambda x: [(int(i[0]), i[1]) for i in x.split(\",\", 1)])\n",
    "# processed_rdd.take(3)\n",
    "# processed_rdd.first()\n",
    "# ads_rdd.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svd_n = 16\n",
    "k = 3\n",
    "model_name = \"kmeans\"\n",
    "\n",
    "\n",
    "hashingTF = HashingTF()\n",
    "tf = hashingTF.transform(filtered_ads)\n",
    "idf = IDF().fit(tf)\n",
    "tfidf = idf.transform(tf)\n",
    "idfIgnore = IDF(minDocFreq=2).fit(tf)\n",
    "tfidf_rdd = idfIgnore.transform(tf)\n",
    "matrix_rdd = RowMatrix(tfidf_rdd)\n",
    "svd_i = matrix_rdd.computeSVD(svd_n, computeU=True)\n",
    "rdd = svd_i.U.rows\n",
    "# clust_model = BisectingKMeans.train(rdd, k)\n",
    "clust_model = KMeans.train(rdd, k)\n",
    "cost = clust_model.computeCost(rdd)\n",
    "save_cluster_metrics(model_name, cost, svd_n, k=k)\n",
    "predictions_rdd = rdd.map(lambda x: (x, clust_model.predict(x)))\n",
    "kmeans_predictions_fn=\"predictions_{}_topics{}_k{}.npy\".format(model_name, svd_n, k)\n",
    "save_cluster_predictions(np.array(predictions_rdd.collect()), model=\"km\", fn=kmeans_predictions_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# run_kmeans_gs(filtered_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_kmeans_gs(rdd, verbose=True):\n",
    "    model = \"bimeans\"\n",
    "    ks = [x for x in range(2, 5, 2)]\n",
    "    svd_topics = [x for x in range(2, 5, 2)]\n",
    "    for topic in svd_topics:\n",
    "        for k in ks:\n",
    "            if verbose: print(\"k:{}, topic:{}\".format(k, topic))\n",
    "            # calculate tfidf scores\n",
    "            tfidf_rdd = get_tfidf(rdd)\n",
    "            \n",
    "            # transform bag of words to svd\n",
    "            # the svd object has U, Sigma, V\n",
    "            # \n",
    "            svd = get_svd(tfidf_rdd, topic)\n",
    "            \n",
    "            # run kmeans with left singular vectors \n",
    "            predictions_rdd = kmeans(svd, topic, k)\n",
    "            kmeans_predictions_fn=\"predictions_{}_topics{}_k{}.npy\".format(model, topic, k)\n",
    "            save_cluster_predictions(np.array(predictions_rdd.collect()), model=\"km\", fn=kmeans_predictions_fn)\n",
    "    \n",
    "# k_means\n",
    "# Build the model (cluster the data)\n",
    "# kmeans(rdd, k, maxIterations, runs, InitializationMode, seed, initializationSteps, epsilon, initialModel)\n",
    "def kmeans(svd, svd_dims, k=2, n_iters=10, save_model=False, verbose=True):\n",
    "    model=\"kmeans\"\n",
    "    if verbose: print(\"in kmeans\")\n",
    "        \n",
    "    # left singular vectors, U\n",
    "    # array of DenseVectors, m_documents x n_topics\n",
    "    # [ doc_i, doc_i+1, ...]\n",
    "    # [[topic_j_score, topic_j+1_score ...], ...]\n",
    "    rdd = svd.U.rows\n",
    "    \n",
    "    # Build the model (cluster the data)\n",
    "    model = KMeans.train(rdd, k, maxIterations=n_iters)\n",
    "\n",
    "    # Evaluate clustering\n",
    "    cost = model.computeCost(rdd)\n",
    "    save_cluster_metrics(\"bimeans\", cost, svd_dims, k=k)\n",
    "    \n",
    "    if save_model:\n",
    "        save_cluster_model(model, fn)\n",
    "    \n",
    "\n",
    "    # returns an rdd of [(topic_values, cluster_id), ...]\n",
    "    return rdd.map(lambda x: (x, model.predict(x)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = run_bimeans_gs(ads_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_bimeans_gs(rdd, verbose=True):\n",
    "    model = \"bimeans\"\n",
    "    predictions = []\n",
    "    ks = [x for x in range(2, 10, 2)]\n",
    "    minDivisibleClusterSize = [float(x/100.0) for x in range(1, 50, 101)], #percent\n",
    "    svd_topics = [x for x in range(2, 10, 2)]\n",
    "    for topic in svd_topics:\n",
    "        for k in ks:\n",
    "            for c_size in minDivisibleClusterSize:\n",
    "                if verbose: print(\"k:{}, topic:{}, cluster_size:{}\".format(k, topic, c_size))\n",
    "                # calculate tfidf scores\n",
    "                tfidf_rdd = get_tfidf(rdd)\n",
    "\n",
    "                # transform bag of words to svd\n",
    "                svd = get_svd(tfidf_rdd, topic)\n",
    "                if verbose: print(svd)\n",
    "\n",
    "                # run kmeans with left singular vectors \n",
    "                predictions_rdd = bimeans(svd, topic, k, c_size)\n",
    "                \n",
    "                predictions.append(predictions_rdd)\n",
    "    return predictions    \n",
    "#     bimeans_predictions_fn=\"predictions_{}_topics{}_k{}_csize{}.npy\".format(model, topic, k, c_size)\n",
    "#     save_cluster_predictions(np.array(predictions), model=\"bimeans\", fn=bimeans_predictions_fn)\n",
    "\n",
    "def bimeans(svd, svd_dims, k, cluster_size, verbose=True, save_model=False):\n",
    "    model = \"bimeans\"\n",
    "    if verbose: print(\"in {}\".format(model))\n",
    "        \n",
    "    # left singular vectors, U\n",
    "    # array of DenseVectors, m_documents x n_topics\n",
    "    # [ doc_i, doc_i+1, ...]\n",
    "    # [[topic_j_score, topic_j+1_score ...], ...]\n",
    "    rdd = svd.U.rows\n",
    "    \n",
    "    # Build the model (cluster the data)\n",
    "#     model = BisectingKMeans.train(rdd, k=k, minDivisibleClusterSize=cluster_size)\n",
    "    model = BisectingKMeans.train(rdd)\n",
    "\n",
    "    # Evaluate clustering\n",
    "    cost = model.computeCost(rdd)\n",
    "    save_cluster_metrics(\"bimeans\", cost, svd_dims, k=k, clust_size=cluster_size)\n",
    "    \n",
    "    if save_model:\n",
    "        save_cluster_model(model, fn)\n",
    "        \n",
    "    # returns an rdd of [(topic_values, cluster_id), ...]\n",
    "    return rdd.map(lambda x: (x, model.predict(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run_lds_gs(ads_rdd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_lda_gs(rdd, verbose=True):\n",
    "\n",
    "    model = \"lda\"\n",
    "    predictions = None\n",
    "    ks = [x for x in range(2, 3, 2)]\n",
    "    doc_concepts = [float(x/100.0) for x in range(1, 10)], # default -1.0\n",
    "    topic_concepts =[float(x/100.0) for x in range(1, 10)] # default -1.0\n",
    "    for k in ks:\n",
    "        for d_concept in doc_concepts:\n",
    "            for t_concept in topic_concepts:\n",
    "                if verbose: print(\"k:{}, d_concept:{}, t_concept:{}\".format(k, d_concept, t_concept))\n",
    "                # calculate tfidf scores\n",
    "                tfidf_rdd = get_tfidf(rdd)\n",
    "\n",
    "                # transform bag of words to svd\n",
    "                svd = get_svd(tfidf_rdd, topic)\n",
    "                if verbose: print(svd)\n",
    "\n",
    "                # run kmeans with left singular vectors \n",
    "                predictions_rdd = bimeans(svd, k, cluster_size)\n",
    "                # first run\n",
    "                if predictions is None:\n",
    "                    #\n",
    "                    predictions = predictions_rdd.collect()\n",
    "                    print(type(predictions))\n",
    "                else:\n",
    "                    predictions.append(predictions_rdd.collect())\n",
    "    bimeans_predictions_fn=\"predictions_{}_topics{}_k{}_csize{}.npz\".format(model, topic, k, c_size)\n",
    "    save_cluster_predictions(np.array(predictions), model=\"bimeans\", fn=kmeans_predictions_fn)\n",
    "\n",
    "def lda(rdd, k, save_model=False):\n",
    "    model = \"lda\"\n",
    "    if verbose: print(\"in {}\", model)\n",
    "        \n",
    "    # Index documents with unique IDs\n",
    "    corpus = rdd.zipWithIndex().map(lambda x: [x[1], x[0]]).cache()\n",
    "\n",
    "    # Cluster the documents into k topics using LDA\n",
    "    model = LDA.train(corpus, k=3)\n",
    "\n",
    "    # Output topics. Each is a distribution over words (matching word count vectors)\n",
    "    print(\"Learned topics (as distributions over vocab of \" + str(ldaModel.vocabSize())\n",
    "          + \" words):\")\n",
    "    topics = model.topicsMatrix()\n",
    "    for topic in range(3):\n",
    "        print(\"Topic \" + str(topic) + \":\")\n",
    "        for word in range(0, ldaModel.vocabSize()):\n",
    "            print(\" \" + str(topics[word][topic]))\n",
    "\n",
    "    # Evaluate clustering\n",
    "    cost = model.computeCost(rdd)\n",
    "    save_cluster_metrics(\"bimeans\", cost, k=k, clust_size=cluster_size)\n",
    "    \n",
    "    if save_model:\n",
    "        save_cluster_model(model, fn)\n",
    "        \n",
    "    # returns an rdd of [(topic_values, cluster_id), ...]\n",
    "    return rdd.map(lambda x: (x, model.predict(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run_gauss_gs(ads_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_gauss_gs(rdd, verbose=True):\n",
    "    model = \"bimeans\"\n",
    "    predictions = None\n",
    "    ks = [x for x in range(2, 3, 2)]\n",
    "    minDivisibleClusterSize = [float(x/100.0) for x in range(1, 50, 50)], #percent\n",
    "    svd_topics = [x for x in range(2, 3, 2)]\n",
    "    for topic in svd_topics:\n",
    "        for k in ks:\n",
    "            for c_size in minDivisibleClusterSize:\n",
    "                if verbose: print(\"k:{}, topic:{}, cluster_size:{}\".format(k, topic, c_size))\n",
    "                # calculate tfidf scores\n",
    "                tfidf_rdd = get_tfidf(rdd)\n",
    "\n",
    "                # transform bag of words to svd\n",
    "                svd = get_svd(tfidf_rdd, topic)\n",
    "                if verbose: print(svd)\n",
    "\n",
    "                # run kmeans with left singular vectors \n",
    "                predictions_rdd = bimeans(svd, k, c_size)\n",
    "                # first run\n",
    "                if predictions is None:\n",
    "                    #\n",
    "                    predictions = predictions_rdd.collect()\n",
    "                    print(type(predictions))\n",
    "                else:\n",
    "                    predictions.append(predictions_rdd.collect())\n",
    "    bimeans_predictions_fn=\"predictions_{}_topics{}_k{}_csize{}.npz\".format(model, topic, k, c_size)\n",
    "    save_cluster_predictions(np.array(predictions), model=\"bimeans\", fn=kmeans_predictions_fn)\n",
    "\n",
    "def gaussian(svd, k, cluster_size, verbose=True, save_model=False):\n",
    "    model = \"bimeans\"\n",
    "    if verbose: print(\"in kmeans\")\n",
    "        \n",
    "    # left singular vectors, U\n",
    "    # array of DenseVectors, m_documents x n_topics\n",
    "    # [ doc_i, doc_i+1, ...]\n",
    "    # [[topic_j_score, topic_j+1_score ...], ...]\n",
    "    rdd = svd.U.rows\n",
    "    \n",
    "    # Build the model (cluster the data)\n",
    "    model = BisectingKMeans.train(rdd, k=k, minDivisibleClusterSize=cluster_size)\n",
    "    # Build the model (cluster the data)\n",
    "    model = GaussianMixture.train(rdd, 2)\n",
    "    \n",
    "    \n",
    "    # Evaluate clustering\n",
    "    cost = model.computeCost(rdd)\n",
    "    save_cluster_metrics(\"bimeans\", cost, k=k, clust_size=cluster_size)\n",
    "    \n",
    "    if save_model:k\n",
    "        save_cluster_model(model, fn)\n",
    "        \n",
    "    # returns an rdd of [(topic_values, cluster_id), ...]\n",
    "    return rdd.map(lambda x: (x, model.predict(x)))\n",
    "\n",
    "\n",
    "def gaussian_clustering(rdd):\n",
    "\n",
    "\n",
    "    # Save and load model\n",
    "    gmm.save(sc, \"target/org/apache/spark/PythonGaussianMixtureExample/GaussianMixtureModel\")\n",
    "    sameModel = GaussianMixtureModel\\\n",
    "        .load(sc, \"target/org/apache/spark/PythonGaussianMixtureExample/GaussianMixtureModel\")\n",
    "\n",
    "    # output parameters of model\n",
    "    for i in range(2):\n",
    "        print(\"weight = \", gmm.weights[i], \"mu = \", gmm.gaussians[i].mu,\n",
    "              \"sigma = \", gmm.gaussians[i].sigma.toArray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "docConcentration or alpha, Concentration parameter (commonly named alpha) for the \n",
    "prior placed on documents distributions over topics (theta)\n",
    "\n",
    "A high alpha-value will lead to documents being more similar in terms of what topics they contain     \n",
    "The effect is based on topic distribution assumption\n",
    "if symmetric distribution - high alpha means that each doc will contain a mix of most topics\n",
    "if symmetric distribution - low alpha means that docs will contain a few topics\n",
    "if asymmetric distribution - vice versa\n",
    "--------------------\n",
    "\n",
    "topicConcentration or beta  Concentration parameter (commonly named beta or eta)\n",
    "for the prior placed on topics distributions over terms. (default: -1.0)\n",
    "\n",
    "A high beta-value will lead to topics being more similar in terms of what words they contain.    \n",
    "if symmetric distribution - high beta means that each doc will contain a mix of most words\n",
    "if symmetric distribution - low alpha means that docs will contain a few words\n",
    "if asymmetric distribution - vice versa\n",
    " \n",
    "\"\"\"\n",
    "cluster_model_params = {\n",
    "   \n",
    "    \"lda\":{\n",
    "        \"k\":[x for x in range(2, 20, 2)],\n",
    "        \"max_iters\": [x for x in range(15, 30, 10)], #default 20  \n",
    "        \"doc_con\":[float(x/100.0) for x in range(1, 10)], # default -1.0\n",
    "        \"topic_con\": [float(x/100.0) for x in range(1, 10)] # default -1.0\n",
    "    }\n",
    "    \"bimeans\":{\n",
    "        \"k\":[x for x in range(2, 20, 2)],\n",
    "        \"max_iters\": [x for x in range(15, 30, 10)], #default 20\n",
    "        \"minDivisibleClusterSize\": [float(x/100.0) for x in range(1, 10)], #percent\n",
    "    }\n",
    "    \"kmeans\":{\n",
    "        \"k\":[x for x in range(2, 20, 2)],\n",
    "        \"max_iters\": [x for x in range(15, 30, 10)], #default 20\n",
    "        \"initializationMode\": [\"random\", \"k-means||\")] #default k-means        \n",
    "    }\n",
    "    \"gaus\":{\n",
    "        \"k\":[x for x in range(2, 20, 2)],\n",
    "        \"max_iters\": [x for x in range(90, 150, 10)], #default 100\n",
    "    }\n",
    "    \"pic\":{\n",
    "        \"k\":[x for x in range(2, 20, 2)],\n",
    "        \"max_iters\": [x for x in range(90, 150, 10)], #default 100\n",
    "    }\n",
    "        \n",
    "}\n",
    "cluster_models = [\"lda\", \"bimeans\", \"kmeans\", \"gaus\", \"pic\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf_rdd.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf_rdd = filtered_rdd.map(lambda x: x[1])\n",
    "hashingTF = HashingTF()\n",
    "tf = hashingTF.transform(filtered_rdd)\n",
    "tf.cache()\n",
    "idf = IDF().fit(tf)\n",
    "tfidf = idf.transform(tf)\n",
    "idfIgnore = IDF(minDocFreq=2).fit(tf)\n",
    "tfidf_rdd = idfIgnore.transform(tf)\n",
    "matrix_rdd = RowMatrix(tfidf_rdd)\n",
    "svd_i = matrix_rdd.computeSVD(5, computeU=True)\n",
    "rdd = svd_i.U.rows\n",
    "clusters = KMeans.train(rdd, 5)\n",
    "\n",
    "#     left singular vectors\n",
    "#     type = RowMatrix\n",
    "#     svd_u = svd.U\n",
    "#     array of DenseVectors, m_documents x n_topics\n",
    "#     [[topic_i, ...], ...]\n",
    "#     return svd_u.rows.collect()\n",
    "# WSSSE = rdd.map(lambda point: error(point)).reduce(lambda x, y: x + y)\n",
    "# print(\"Within Set Sum of Squared Error = \" + str(WSSSE))\n",
    "# predicts = rdd.map(lambda x: (x, clusters.predict(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_bimeans_gs(rdd, verbose=True):\n",
    "    predictions = None\n",
    "    ks = [x for x in range(2, 20, 2)]\n",
    "    minDivisibleClusterSize: [float(x/100.0) for x in range(1, 50,10)], #percent\n",
    "    svd_topics = [x for x in range(2, 10, 2)]\n",
    "    for topic in svd_topics:\n",
    "        for k in ks:\n",
    "            # calculate tfidf scores\n",
    "            tfidf_rdd = get_tfidf(rdd)\n",
    "            \n",
    "            # transform bag of words to svd\n",
    "            svd = get_svd(tfidf_rdd, topic)\n",
    "            if verbose: print(svd)\n",
    "            \n",
    "            # run kmeans with left singular vectors \n",
    "            predictions_rdd = bimeans(svd, k)\n",
    "            if predictions is None:\n",
    "                predictions = predictions_rdd.collect()\n",
    "                print(type(predictions))\n",
    "            else:\n",
    "                predictions.append(predictions_rdd.collect())\n",
    "    kmeans_predictions_fn=\"predictions_k{}_topics{}.npz\"\n",
    "    save_cluster_predictions(np.array(predictions), model=\"km\", fn=kmeans_predictions_fn)\n",
    "    save_cluster_predictions(np.array(predictions), model=\"km\", fn=kmeans_predictions_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def transpose_rdd()\n",
    "# tfidf_rdd.flatMap(lambda x: x).take(3)\n",
    "# flatMap by keeping the column position\n",
    "# flat_rdd = tfidf_rdd.flatMap(lambda row: row.map(lambda col: (col, row.indexOf(col))))\n",
    "# flat_rdd.take(3)\n",
    "# .map(v => (v._2, v._1)) // key by column position\n",
    "# .groupByKey.sortByKey   // regroup on column position, thus all elements from the first column will be in the first row\n",
    "# .map(_._2)              // discard the key, keep only value\n",
    "# df = rdd.toDF()\n",
    "# # Grab data from first columns, since it will be transposed to new column headers\n",
    "# new_header = [i[0] for i in dt.select(\"_1\").rdd.map(tuple).collect()]\n",
    "\n",
    "# # Remove first column from dataframe\n",
    "# dt2 = dt.select([c for c in dt.columns if c not in ['_1']])\n",
    "\n",
    "# # Convert DataFrame to RDD\n",
    "# rdd = dt2.rdd.map(tuple)\n",
    "\n",
    "# # Transpose Data\n",
    "# rddT1 = rdd.zipWithIndex().flatMap(lambda (x,i): [(i,j,e) for (j,e) in enumerate(x)])\n",
    "# rddT2 = rddT1.map(lambda (i,j,e): (j, (i,e))).groupByKey().sortByKey()\n",
    "# rddT3 = rddT2.map(lambda (i, x): sorted(list(x), cmp=lambda (i1,e1),(i2,e2) : cmp(i1, i2)))\n",
    "# rddT4 = rddT3.map(lambda x: map(lambda (i, y): y , x))\n",
    "\n",
    "# # Convert back to DataFrame (along with header)\n",
    "# df = rddT4.toDF(new_header)\n",
    "\n",
    "# return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "docConcentration or alpha, Concentration parameter (commonly named alpha) for the \n",
    "prior placed on documents distributions over topics (theta)\n",
    "\n",
    "A high alpha-value will lead to documents being more similar in terms of what topics they contain     \n",
    "The effect is based on topic distribution assumption\n",
    "if symmetric distribution - high alpha means that each doc will contain a mix of most topics\n",
    "if symmetric distribution - low alpha means that docs will contain a few topics\n",
    "if asymmetric distribution - vice versa\n",
    "--------------------\n",
    "\n",
    "topicConcentration or beta  Concentration parameter (commonly named beta or eta)\n",
    "for the prior placed on topics distributions over terms. (default: -1.0)\n",
    "\n",
    "A high beta-value will lead to topics being more similar in terms of what words they contain.    \n",
    "if symmetric distribution - high beta means that each doc will contain a mix of most words\n",
    "if symmetric distribution - low alpha means that docs will contain a few words\n",
    "if asymmetric distribution - vice versa\n",
    " \n",
    "\"\"\"\n",
    "cluster_model_params = {\n",
    "   \n",
    "    \"lda\":{\n",
    "        \"k\":[x for x in range(2, 20, 2)],\n",
    "        \"max_iters\": [x for x in range(15, 30, 10)], #default 20  \n",
    "        \"doc_con\":[float(x/100.0) for x in range(1, 10)], # default -1.0\n",
    "        \"topic_con\": [float(x/100.0) for x in range(1, 10)] # default -1.0\n",
    "    }\n",
    "    \"bimeans\":{\n",
    "        \"k\":[x for x in range(2, 20, 2)],\n",
    "        \"max_iters\": [x for x in range(15, 30, 10)], #default 20\n",
    "        \"minDivisibleClusterSize\": [float(x/100.0) for x in range(1, 10)], #percent\n",
    "    }\n",
    "    \"kmeans\":{\n",
    "        \"k\":[x for x in range(2, 20, 2)],\n",
    "        \"max_iters\": [x for x in range(15, 30, 10)], #default 20\n",
    "        \"initializationMode\": [\"random\", \"k-means||\")] #default k-means        \n",
    "    }\n",
    "    \"gaus\":{\n",
    "        \"k\":[x for x in range(2, 20, 2)],\n",
    "        \"max_iters\": [x for x in range(90, 150, 10)], #default 100\n",
    "    }\n",
    "    \"pic\":{\n",
    "        \"k\":[x for x in range(2, 20, 2)],\n",
    "        \"max_iters\": [x for x in range(90, 150, 10)], #default 100\n",
    "    }\n",
    "        \n",
    "}\n",
    "cluster_models = [\"lda\", \"bimeans\", \"kmeans\", \"gaus\", \"pic\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
