{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# general pyspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n",
    "\n",
    "# conf = SparkConf().setMaster(\"local\").setAppName(\"svd_cluster.py\")\n",
    "# sc = SparkContext(conf = conf)\n",
    "\n",
    "#import mllib\n",
    "from pyspark.mllib.evaluation import RegressionMetrics\n",
    "from pyspark.mllib.feature import HashingTF, IDF\n",
    "from pyspark.mllib.feature import Word2Vec\n",
    "from pyspark.mllib.feature import StandardScalerModel\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.linalg.distributed import RowMatrix, BlockMatrix\n",
    "from pyspark.mllib.linalg import Matrices\n",
    "from pyspark.mllib.clustering import KMeans, KMeansModel\n",
    "from pyspark.mllib.clustering import LDA, LDAModel\n",
    "from pyspark.mllib.clustering import BisectingKMeans, BisectingKMeansModel\n",
    "\n",
    "# python imports\n",
    "from math import sqrt\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "import os, csv, sys, time\n",
    "from random import randint\n",
    "from itertools import izip, izip_longest\n",
    "\n",
    "sc = SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shane\\programming\\cs657_mining_massive_datasets\\craigslist_clustering\\data\\cl_tiny.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# def get_input(fn):\n",
    "\n",
    "fn = \"cl_tiny.csv\"\n",
    "cur_dir = os.path.abspath(os.curdir)\n",
    "input_file_path = os.path.normpath(os.path.join(cur_dir, \"..\", \"data\", fn))\n",
    "print(input_file_path)\n",
    "\n",
    "os.path.isfile(input_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "docConcentration or alpha, Concentration parameter (commonly named “alpha”) for the \n",
    "prior placed on documents’ distributions over topics (“theta”)\n",
    "\n",
    "A high alpha-value will lead to documents being more similar in terms of what topics they contain     \n",
    "The effect is based on topic distribution assumption\n",
    "if symmetric distribution - high alpha means that each doc will contain a mix of most topics\n",
    "if symmetric distribution - low alpha means that docs will contain a few topics\n",
    "if asymmetric distribution - vice versa\n",
    "--------------------\n",
    "\n",
    "topicConcentration or beta – Concentration parameter (commonly named “beta” or “eta”)\n",
    "for the prior placed on topics’ distributions over terms. (default: -1.0)\n",
    "\n",
    "A high beta-value will lead to topics being more similar in terms of what words they contain.    \n",
    "if symmetric distribution - high beta means that each doc will contain a mix of most words\n",
    "if symmetric distribution - low alpha means that docs will contain a few words\n",
    "if asymmetric distribution - vice versa\n",
    " \n",
    "\"\"\"\n",
    "cluster_model_params = {\n",
    "   \n",
    "    \"lda\":{\n",
    "        \"k\":[x for x in range(2, 20, 2)],\n",
    "        \"max_iters\": [x for x in range(15, 30, 10)], #default 20  \n",
    "        \"doc_con\":[float(x/100.0) for x in range(1, 10)], # default -1.0\n",
    "        \"topic_con\": [float(x/100.0) for x in range(1, 10)] # default -1.0\n",
    "    }\n",
    "    \"bimeans\":{\n",
    "        \"k\":[x for x in range(2, 20, 2)],\n",
    "        \"max_iters\": [x for x in range(15, 30, 10)], #default 20\n",
    "        \"minDivisibleClusterSize\": [float(x/100.0) for x in range(1, 10)], #percent\n",
    "    }\n",
    "    \"kmeans\":{\n",
    "        \"k\":[x for x in range(2, 20, 2)],\n",
    "        \"max_iters\": [x for x in range(15, 30, 10)], #default 20\n",
    "        \"initializationMode\": [\"random\", \"k-means||\")] #default k-means        \n",
    "    }\n",
    "    \"gaus\":{\n",
    "        \"k\":[x for x in range(2, 20, 2)],\n",
    "        \"max_iters\": [x for x in range(90, 150, 10)], #default 100\n",
    "    }\n",
    "    \"pic\":{\n",
    "        \"k\":[x for x in range(2, 20, 2)],\n",
    "        \"max_iters\": [x for x in range(90, 150, 10)], #default 100\n",
    "    }\n",
    "        \n",
    "}\n",
    "cluster_models = [\"lda\", \"bimeans\", \"kmeans\", \"gaus\", \"pic\"]\n",
    "\n",
    "# processed_rdd = input.map(lambda x: str(x.decode('utf-8', 'ignore'))).map(lambda x: x.split(\",\"))\n",
    "# processed_rdd.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# [(postTitle, postingURL, postLocation, time, lat, long, address, dateRetrieved, post_date, ad), ...]\n",
    "# tiny input has 30 reviews\n",
    "raw_ads = sc.textFile(input_file_path)\n",
    "# set = input.take(3)\n",
    "# [ad0, ad1, ..]\n",
    "ads_rdd = raw_ads.map(lambda x: str(x.decode('utf-8', 'ignore')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(ads_rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashingTF = HashingTF()\n",
    "tf = hashingTF.transform(ads_rdd)\n",
    "tf.cache()\n",
    "idf = IDF().fit(tf)\n",
    "tfidf = idf.transform(tf)\n",
    "# spark.mllib's IDF implementation provides an option for ignoring terms\n",
    "# which occur in less than a minimum number of documents.\n",
    "# In such cases, the IDF for these terms is set to 0.\n",
    "# This feature can be used by passing the minDocFreq value to the IDF constructor.\n",
    "idfIgnore = IDF(minDocFreq=1).fit(tf)\n",
    "tfidf_rdd = idfIgnore.transform(tf)\n",
    "matrix_rdd = RowMatrix(tfidf_rdd)\n",
    "#     left singular vectors\n",
    "#     type = RowMatrix\n",
    "#     svd_u = svd.U\n",
    "#     array of DenseVectors, m_documents x n_topics\n",
    "#     [[topic_i, ...], ...]\n",
    "#     return svd_u.rows.collect()\n",
    "svd_i = matrix_rdd.computeSVD(3, computeU=True)\n",
    "rdd = svd_i.U.rows\n",
    "clusters = KMeans.train(rdd, 5)\n",
    "# WSSSE = rdd.map(lambda point: error(point)).reduce(lambda x, y: x + y)\n",
    "# print(\"Within Set Sum of Squared Error = \" + str(WSSSE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicts = rdd.map(lambda x: (x, clusters.predict(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(DenseVector([0.1343, 0.0476, 0.021]), 4),\n",
       " (DenseVector([0.2498, 0.0151, 0.3793]), 0),\n",
       " (DenseVector([0.0404, 0.0287, 0.0503]), 3)]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-100-b66117a46539>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-100-b66117a46539>\"\u001b[1;36m, line \u001b[1;32m4\u001b[0m\n\u001b[1;33m    minDivisibleClusterSize: [float(x/100.0) for x in range(1, 50,10)], #percent\u001b[0m\n\u001b[1;37m                           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run_kmeans_gs(ads_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_kmeans_gs(rdd, verbose=True):\n",
    "    predictions = \n",
    "    ks = [x for x in range(2, 10, 2)]\n",
    "    svd_topics = [x for x in range(2, 10, 2)]\n",
    "    for topic in svd_topics:\n",
    "        for k in ks:\n",
    "            if verbose: print(\"k:{}, topic:{}\".format(k, topic))\n",
    "            # calculate tfidf scores\n",
    "            tfidf_rdd = get_tfidf(rdd)\n",
    "            \n",
    "            # transform bag of words to svd\n",
    "            # the svd object has U, Sigma, V\n",
    "            # \n",
    "            svd = get_svd(tfidf_rdd, topic)\n",
    "            \n",
    "            # run kmeans with left singular vectors \n",
    "            predictions_rdd = kmeans(svd, k)\n",
    "            if predictions is None:\n",
    "                # list of vectors, where each vector is a document and its topic scores\n",
    "                # [DenseVector[dim_i, dim_i+1 ...], ...]\n",
    "                predictions = predictions_rdd.collect()\n",
    "                print(type(predictions))\n",
    "            else:\n",
    "                predictions.append(predictions_rdd.collect())\n",
    "    kmeans_predictions_fn=\"predictions_k{}_topics{}.npz\"\n",
    "    save_cluster_predictions(np.array(predictions), model=\"km\", fn=kmeans_predictions_fn)\n",
    "    \n",
    "# k_means\n",
    "# Build the model (cluster the data)\n",
    "# kmeans(rdd, k, maxIterations, runs, InitializationMode, seed, initializationSteps, epsilon, initialModel)\n",
    "def kmeans(svd, k=2, n_iters=10, save_model=False, verbose=True):\n",
    "    model=\"kmeans\"\n",
    "    if verbose: print(\"in kmeans\")\n",
    "        \n",
    "    # left singular vectors, U\n",
    "    # array of DenseVectors, m_documents x n_topics\n",
    "    # [ doc_i, doc_i+1, ...]\n",
    "    # [[topic_j_score, topic_j+1_score ...], ...]\n",
    "    rdd = svd.U.rows\n",
    "    \n",
    "    # Build the model (cluster the data)\n",
    "    model = KMeans.train(rdd, k, maxIterations=n_iters)\n",
    "\n",
    "    # Evaluate clustering\n",
    "    cost = model.computeCost(rdd)\n",
    "    save_cluster_metrics(\"bimeans\", cost, k=k, clust_size=cluster_size)\n",
    "    \n",
    "    if save_model:\n",
    "        save_cluster_model(model, fn)\n",
    "    \n",
    "\n",
    "    # returns an rdd of [(topic_values, cluster_id), ...]\n",
    "    return rdd.map(lambda x: (x, model.predict(x)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k:2, topic:2, cluster_size:[0.01]\n",
      "in get_tfidf\n",
      "in get_svd\n",
      "<pyspark.mllib.linalg.distributed.SingularValueDecomposition object at 0x0000000007F79908>\n",
      "in kmeans\n",
      "saving cluster metrics to csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\opt\\spark\\spark-2.2.0-bin-hadoop2.7\\python\\pyspark\\cloudpickle.py\", line 148, in dump\n",
      "    return Pickler.dump(self, obj)\n",
      "  File \"C:\\Users\\shane\\Anaconda3\\envs\\py27\\lib\\pickle.py\", line 224, in dump\n",
      "    self.save(obj)\n",
      "  File \"C:\\Users\\shane\\Anaconda3\\envs\\py27\\lib\\pickle.py\", line 286, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"C:\\Users\\shane\\Anaconda3\\envs\\py27\\lib\\pickle.py\", line 568, in save_tuple\n",
      "    save(element)\n",
      "  File \"C:\\Users\\shane\\Anaconda3\\envs\\py27\\lib\\pickle.py\", line 286, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"C:\\opt\\spark\\spark-2.2.0-bin-hadoop2.7\\python\\pyspark\\cloudpickle.py\", line 255, in save_function\n",
      "    self.save_function_tuple(obj)\n",
      "  File \"C:\\opt\\spark\\spark-2.2.0-bin-hadoop2.7\\python\\pyspark\\cloudpickle.py\", line 292, in save_function_tuple\n",
      "    save((code, closure, base_globals))\n",
      "  File \"C:\\Users\\shane\\Anaconda3\\envs\\py27\\lib\\pickle.py\", line 286, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"C:\\Users\\shane\\Anaconda3\\envs\\py27\\lib\\pickle.py\", line 554, in save_tuple\n",
      "    save(element)\n",
      "  File \"C:\\Users\\shane\\Anaconda3\\envs\\py27\\lib\\pickle.py\", line 286, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"C:\\Users\\shane\\Anaconda3\\envs\\py27\\lib\\pickle.py\", line 606, in save_list\n",
      "    self._batch_appends(iter(obj))\n",
      "  File \"C:\\Users\\shane\\Anaconda3\\envs\\py27\\lib\\pickle.py\", line 642, in _batch_appends\n",
      "    save(tmp[0])\n",
      "  File \"C:\\Users\\shane\\Anaconda3\\envs\\py27\\lib\\pickle.py\", line 286, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"C:\\opt\\spark\\spark-2.2.0-bin-hadoop2.7\\python\\pyspark\\cloudpickle.py\", line 249, in save_function\n",
      "    self.save_function_tuple(obj)\n",
      "  File \"C:\\opt\\spark\\spark-2.2.0-bin-hadoop2.7\\python\\pyspark\\cloudpickle.py\", line 292, in save_function_tuple\n",
      "    save((code, closure, base_globals))\n",
      "  File \"C:\\Users\\shane\\Anaconda3\\envs\\py27\\lib\\pickle.py\", line 286, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"C:\\Users\\shane\\Anaconda3\\envs\\py27\\lib\\pickle.py\", line 554, in save_tuple\n",
      "    save(element)\n",
      "  File \"C:\\Users\\shane\\Anaconda3\\envs\\py27\\lib\\pickle.py\", line 286, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"C:\\Users\\shane\\Anaconda3\\envs\\py27\\lib\\pickle.py\", line 606, in save_list\n",
      "    self._batch_appends(iter(obj))\n",
      "  File \"C:\\Users\\shane\\Anaconda3\\envs\\py27\\lib\\pickle.py\", line 642, in _batch_appends\n",
      "    save(tmp[0])\n",
      "  File \"C:\\Users\\shane\\Anaconda3\\envs\\py27\\lib\\pickle.py\", line 331, in save\n",
      "    self.save_reduce(obj=obj, *rv)\n",
      "  File \"C:\\opt\\spark\\spark-2.2.0-bin-hadoop2.7\\python\\pyspark\\cloudpickle.py\", line 600, in save_reduce\n",
      "    save(state)\n",
      "  File \"C:\\Users\\shane\\Anaconda3\\envs\\py27\\lib\\pickle.py\", line 286, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"C:\\Users\\shane\\Anaconda3\\envs\\py27\\lib\\pickle.py\", line 655, in save_dict\n",
      "    self._batch_setitems(obj.iteritems())\n",
      "  File \"C:\\Users\\shane\\Anaconda3\\envs\\py27\\lib\\pickle.py\", line 687, in _batch_setitems\n",
      "    save(v)\n",
      "  File \"C:\\Users\\shane\\Anaconda3\\envs\\py27\\lib\\pickle.py\", line 306, in save\n",
      "    rv = reduce(self.proto)\n",
      "  File \"C:\\opt\\spark\\spark-2.2.0-bin-hadoop2.7\\python\\pyspark\\context.py\", line 306, in __getnewargs__\n",
      "    \"It appears that you are attempting to reference SparkContext from a broadcast \"\n",
      "Exception: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.\n"
     ]
    },
    {
     "ename": "PicklingError",
     "evalue": "Could not serialize object: Exception: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPicklingError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-178-c7c323de0446>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mrun_bimeans_gs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mads_rdd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-177-db93c18c9226>\u001b[0m in \u001b[0;36mrun_bimeans_gs\u001b[1;34m(rdd, verbose)\u001b[0m\n\u001b[0;32m     21\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m                     \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m                     \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredictions_rdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m                     \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\opt\\spark\\spark-2.2.0-bin-hadoop2.7\\python\\pyspark\\rdd.pyc\u001b[0m in \u001b[0;36mcollect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    807\u001b[0m         \"\"\"\n\u001b[0;32m    808\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 809\u001b[1;33m             \u001b[0mport\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    810\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mport\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    811\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\opt\\spark\\spark-2.2.0-bin-hadoop2.7\\python\\pyspark\\rdd.pyc\u001b[0m in \u001b[0;36m_jrdd\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2453\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2454\u001b[0m         wrapped_func = _wrap_function(self.ctx, self.func, self._prev_jrdd_deserializer,\n\u001b[1;32m-> 2455\u001b[1;33m                                       self._jrdd_deserializer, profiler)\n\u001b[0m\u001b[0;32m   2456\u001b[0m         python_rdd = self.ctx._jvm.PythonRDD(self._prev_jrdd.rdd(), wrapped_func,\n\u001b[0;32m   2457\u001b[0m                                              self.preservesPartitioning)\n",
      "\u001b[1;32mC:\\opt\\spark\\spark-2.2.0-bin-hadoop2.7\\python\\pyspark\\rdd.pyc\u001b[0m in \u001b[0;36m_wrap_function\u001b[1;34m(sc, func, deserializer, serializer, profiler)\u001b[0m\n\u001b[0;32m   2386\u001b[0m     \u001b[1;32massert\u001b[0m \u001b[0mserializer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"serializer should not be empty\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2387\u001b[0m     \u001b[0mcommand\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprofiler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdeserializer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mserializer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2388\u001b[1;33m     \u001b[0mpickled_command\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbroadcast_vars\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mincludes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_prepare_for_python_RDD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2389\u001b[0m     return sc._jvm.PythonFunction(bytearray(pickled_command), env, includes, sc.pythonExec,\n\u001b[0;32m   2390\u001b[0m                                   sc.pythonVer, broadcast_vars, sc._javaAccumulator)\n",
      "\u001b[1;32mC:\\opt\\spark\\spark-2.2.0-bin-hadoop2.7\\python\\pyspark\\rdd.pyc\u001b[0m in \u001b[0;36m_prepare_for_python_RDD\u001b[1;34m(sc, command)\u001b[0m\n\u001b[0;32m   2372\u001b[0m     \u001b[1;31m# the serialized command will be compressed by broadcast\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2373\u001b[0m     \u001b[0mser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCloudPickleSerializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2374\u001b[1;33m     \u001b[0mpickled_command\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2375\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpickled_command\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m<<\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# 1M\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2376\u001b[0m         \u001b[1;31m# The broadcast will have same life cycle as created PythonRDD\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\opt\\spark\\spark-2.2.0-bin-hadoop2.7\\python\\pyspark\\serializers.pyc\u001b[0m in \u001b[0;36mdumps\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    458\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdumps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 460\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcloudpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    461\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    462\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\opt\\spark\\spark-2.2.0-bin-hadoop2.7\\python\\pyspark\\cloudpickle.pyc\u001b[0m in \u001b[0;36mdumps\u001b[1;34m(obj, protocol)\u001b[0m\n\u001b[0;32m    702\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    703\u001b[0m     \u001b[0mcp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCloudPickler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 704\u001b[1;33m     \u001b[0mcp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    705\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    706\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\opt\\spark\\spark-2.2.0-bin-hadoop2.7\\python\\pyspark\\cloudpickle.pyc\u001b[0m in \u001b[0;36mdump\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    160\u001b[0m                 \u001b[0mmsg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Could not serialize object: %s: %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0memsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m             \u001b[0mprint_exec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 162\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPicklingError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    163\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msave_memoryview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPicklingError\u001b[0m: Could not serialize object: Exception: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063."
     ]
    }
   ],
   "source": [
    "run_bimeans_gs(ads_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_bimeans_gs(rdd, verbose=True):\n",
    "    model = \"bimeans\"\n",
    "    predictions = None\n",
    "    ks = [x for x in range(2, 3, 2)]\n",
    "    minDivisibleClusterSize = [float(x/100.0) for x in range(1, 50, 50)], #percent\n",
    "    svd_topics = [x for x in range(2, 3, 2)]\n",
    "    for topic in svd_topics:\n",
    "        for k in ks:\n",
    "            for c_size in minDivisibleClusterSize:\n",
    "                if verbose: print(\"k:{}, topic:{}, cluster_size:{}\".format(k, topic, c_size))\n",
    "                # calculate tfidf scores\n",
    "                tfidf_rdd = get_tfidf(rdd)\n",
    "\n",
    "                # transform bag of words to svd\n",
    "                svd = get_svd(tfidf_rdd, topic)\n",
    "                if verbose: print(svd)\n",
    "\n",
    "                # run kmeans with left singular vectors \n",
    "                predictions_rdd = bimeans(svd, k, c_size)\n",
    "                # first run\n",
    "                if predictions is None:\n",
    "                    #\n",
    "                    predictions = predictions_rdd.collect()\n",
    "                    print(type(predictions))\n",
    "                else:\n",
    "                    predictions.append(predictions_rdd.collect())\n",
    "    bimeans_predictions_fn=\"predictions_{}_topics{}_k{}_csize{}.npz\".format(model, topic, k, c_size)\n",
    "    save_cluster_predictions(np.array(predictions), model=\"bimeans\", fn=kmeans_predictions_fn)\n",
    "\n",
    "def bimeans(svd, k, cluster_size, verbose=True, save_model=False):\n",
    "    model = \"bimeans\"\n",
    "    if verbose: print(\"in kmeans\")\n",
    "        \n",
    "    # left singular vectors, U\n",
    "    # array of DenseVectors, m_documents x n_topics\n",
    "    # [ doc_i, doc_i+1, ...]\n",
    "    # [[topic_j_score, topic_j+1_score ...], ...]\n",
    "    rdd = svd.U.rows\n",
    "    \n",
    "    # Build the model (cluster the data)\n",
    "#     model = BisectingKMeans.train(rdd, k=k, minDivisibleClusterSize=cluster_size)\n",
    "    model = BisectingKMeans.train(rdd)\n",
    "\n",
    "    # Evaluate clustering\n",
    "    cost = model.computeCost(rdd)\n",
    "    save_cluster_metrics(\"bimeans\", cost, k=k, clust_size=cluster_size)\n",
    "    \n",
    "    if save_model:\n",
    "        save_cluster_model(model, fn)\n",
    "        \n",
    "    # returns an rdd of [(topic_values, cluster_id), ...]\n",
    "    return rdd.map(lambda x: (x, model.predict(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run_lds_gs(ads_rdd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_lda_gs(rdd, verbose=True):\n",
    "\n",
    "    model = \"lda\"\n",
    "    predictions = None\n",
    "    ks = [x for x in range(2, 3, 2)]\n",
    "    doc_concepts = [float(x/100.0) for x in range(1, 10)], # default -1.0\n",
    "    topic_concepts =[float(x/100.0) for x in range(1, 10)] # default -1.0\n",
    "    for k in ks:\n",
    "        for d_concept in doc_concepts:\n",
    "            for t_concept in topic_concepts:\n",
    "                if verbose: print(\"k:{}, d_concept:{}, t_concept:{}\".format(k, d_concept, t_concept))\n",
    "                # calculate tfidf scores\n",
    "                tfidf_rdd = get_tfidf(rdd)\n",
    "\n",
    "                # transform bag of words to svd\n",
    "                svd = get_svd(tfidf_rdd, topic)\n",
    "                if verbose: print(svd)\n",
    "\n",
    "                # run kmeans with left singular vectors \n",
    "                predictions_rdd = bimeans(svd, k, cluster_size)\n",
    "                # first run\n",
    "                if predictions is None:\n",
    "                    #\n",
    "                    predictions = predictions_rdd.collect()\n",
    "                    print(type(predictions))\n",
    "                else:\n",
    "                    predictions.append(predictions_rdd.collect())\n",
    "    bimeans_predictions_fn=\"predictions_{}_topics{}_k{}_csize{}.npz\".format(model, topic, k, c_size)\n",
    "    save_cluster_predictions(np.array(predictions), model=\"bimeans\", fn=kmeans_predictions_fn)\n",
    "\n",
    "def lda(rdd, k, save_model=False):\n",
    "    model = \"lda\"\n",
    "    if verbose: print(\"in {}\", model)\n",
    "        \n",
    "    # Index documents with unique IDs\n",
    "    corpus = rdd.zipWithIndex().map(lambda x: [x[1], x[0]]).cache()\n",
    "\n",
    "    # Cluster the documents into k topics using LDA\n",
    "    model = LDA.train(corpus, k=3)\n",
    "\n",
    "    # Output topics. Each is a distribution over words (matching word count vectors)\n",
    "    print(\"Learned topics (as distributions over vocab of \" + str(ldaModel.vocabSize())\n",
    "          + \" words):\")\n",
    "    topics = model.topicsMatrix()\n",
    "    for topic in range(3):\n",
    "        print(\"Topic \" + str(topic) + \":\")\n",
    "        for word in range(0, ldaModel.vocabSize()):\n",
    "            print(\" \" + str(topics[word][topic]))\n",
    "\n",
    "    # Evaluate clustering\n",
    "    cost = model.computeCost(rdd)\n",
    "    save_cluster_metrics(\"bimeans\", cost, k=k, clust_size=cluster_size)\n",
    "    \n",
    "    if save_model:\n",
    "        save_cluster_model(model, fn)\n",
    "        \n",
    "    # returns an rdd of [(topic_values, cluster_id), ...]\n",
    "    return rdd.map(lambda x: (x, model.predict(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run_gauss_gs(ads_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_gauss_gs(rdd, verbose=True):\n",
    "    model = \"bimeans\"\n",
    "    predictions = None\n",
    "    ks = [x for x in range(2, 3, 2)]\n",
    "    minDivisibleClusterSize = [float(x/100.0) for x in range(1, 50, 50)], #percent\n",
    "    svd_topics = [x for x in range(2, 3, 2)]\n",
    "    for topic in svd_topics:\n",
    "        for k in ks:\n",
    "            for c_size in minDivisibleClusterSize:\n",
    "                if verbose: print(\"k:{}, topic:{}, cluster_size:{}\".format(k, topic, c_size))\n",
    "                # calculate tfidf scores\n",
    "                tfidf_rdd = get_tfidf(rdd)\n",
    "\n",
    "                # transform bag of words to svd\n",
    "                svd = get_svd(tfidf_rdd, topic)\n",
    "                if verbose: print(svd)\n",
    "\n",
    "                # run kmeans with left singular vectors \n",
    "                predictions_rdd = bimeans(svd, k, c_size)\n",
    "                # first run\n",
    "                if predictions is None:\n",
    "                    #\n",
    "                    predictions = predictions_rdd.collect()\n",
    "                    print(type(predictions))\n",
    "                else:\n",
    "                    predictions.append(predictions_rdd.collect())\n",
    "    bimeans_predictions_fn=\"predictions_{}_topics{}_k{}_csize{}.npz\".format(model, topic, k, c_size)\n",
    "    save_cluster_predictions(np.array(predictions), model=\"bimeans\", fn=kmeans_predictions_fn)\n",
    "\n",
    "def gaussian(svd, k, cluster_size, verbose=True, save_model=False):\n",
    "    model = \"bimeans\"\n",
    "    if verbose: print(\"in kmeans\")\n",
    "        \n",
    "    # left singular vectors, U\n",
    "    # array of DenseVectors, m_documents x n_topics\n",
    "    # [ doc_i, doc_i+1, ...]\n",
    "    # [[topic_j_score, topic_j+1_score ...], ...]\n",
    "    rdd = svd.U.rows\n",
    "    \n",
    "    # Build the model (cluster the data)\n",
    "    model = BisectingKMeans.train(rdd, k=k, minDivisibleClusterSize=cluster_size)\n",
    "    # Build the model (cluster the data)\n",
    "    model = GaussianMixture.train(rdd, 2)\n",
    "    \n",
    "    \n",
    "    # Evaluate clustering\n",
    "    cost = model.computeCost(rdd)\n",
    "    save_cluster_metrics(\"bimeans\", cost, k=k, clust_size=cluster_size)\n",
    "    \n",
    "    if save_model:\n",
    "        save_cluster_model(model, fn)\n",
    "        \n",
    "    # returns an rdd of [(topic_values, cluster_id), ...]\n",
    "    return rdd.map(lambda x: (x, model.predict(x)))\n",
    "\n",
    "\n",
    "def gaussian_clustering(rdd):\n",
    "\n",
    "\n",
    "    # Save and load model\n",
    "    gmm.save(sc, \"target/org/apache/spark/PythonGaussianMixtureExample/GaussianMixtureModel\")\n",
    "    sameModel = GaussianMixtureModel\\\n",
    "        .load(sc, \"target/org/apache/spark/PythonGaussianMixtureExample/GaussianMixtureModel\")\n",
    "\n",
    "    # output parameters of model\n",
    "    for i in range(2):\n",
    "        print(\"weight = \", gmm.weights[i], \"mu = \", gmm.gaussians[i].mu,\n",
    "              \"sigma = \", gmm.gaussians[i].sigma.toArray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_tfidf(rdd, verbose=True):\n",
    "    if verbose: print(\"in get_tfidf\")\n",
    "    # While applying HashingTF only needs a single pass to the data, applying IDF needs two passes:\n",
    "    # First to compute the IDF vector and second to scale the term frequencies by IDF.\n",
    "    hashingTF = HashingTF()\n",
    "    tf = hashingTF.transform(rdd)\n",
    "    tf.cache()\n",
    "    idf = IDF().fit(tf)\n",
    "    tfidf = idf.transform(tf)\n",
    "    # spark.mllib's IDF implementation provides an option for ignoring terms\n",
    "    # which occur in less than a minimum number of documents.\n",
    "    # In such cases, the IDF for these terms is set to 0.\n",
    "    # This feature can be used by passing the minDocFreq value to the IDF constructor.\n",
    "    idfIgnore = IDF(minDocFreq=1).fit(tf)\n",
    "    tfidf_rdd = idfIgnore.transform(tf)\n",
    "    # rdd of SparseVectors [(doc_id_i: {word_id_j: tfidfscore_j, ...}), ... }]\n",
    "    # or m docs x n counts\n",
    "    return tfidf_rdd\n",
    "\n",
    "def get_svd(tfidf_rdd, n_topics=3, verbose=True):\n",
    "    if verbose: print(\"in get_svd\")\n",
    "    # distributed matrix\n",
    "    matrix_rdd = RowMatrix(tfidf_rdd)\n",
    "#     left singular vectors\n",
    "#     type = RowMatrix\n",
    "#     svd_u = svd.U\n",
    "#     array of DenseVectors, m_documents x n_topics\n",
    "#     [[topic_i, ...], ...]\n",
    "#     return svd_u.rows.collect()\n",
    "    svd = matrix_rdd.computeSVD(n_topics, computeU=True)\n",
    "    return svd\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_cluster_metrics(model, score, k=None, max_iters=None, clust_size=None, doc_concept=None, topic_concept=None):\n",
    "    print('saving cluster metrics to csv')\n",
    "    row = [model, score, k, max_iters, clust_size, doc_concept, topic_concept]\n",
    "    with open(fn, 'a+') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_svd_U(svd_U_rdd, fn=\"svd_u_results.npz\"):\n",
    "    np.savez(fn,np.array(svd_U_rdd))\n",
    "\n",
    "#     sentence = \"aa bb ab\" * 10 + \"a cd \" * 10\n",
    "#     localDoc = [sentence, sentence]\n",
    "#     doc = sc.parallelize(localDoc).map(lambda line: line.split(\" \"))\n",
    "#     model = Word2Vec().setVectorSize(10).setSeed(42).fit(doc)\n",
    "# i think it is expecting a list of document lists [[word1, word2,...], ...]\n",
    "def get_word2vec(rdd):\n",
    "    word2vec = Word2Vec()\n",
    "    model = word2vec.fit(ads_rdd)\n",
    "\n",
    "def save_cluster_predictions(cluster_results, model=\"km\", fn=\"cluster_results.pkl\"):\n",
    "    results_fn = \"{}_{}\".format(model, fn)\n",
    "    np.savez(results_fn, cluster_results_rdd.collect())\n",
    "\n",
    "# Evaluate clustering by computing Within Set Sum of Squared Errors\n",
    "def error(point):\n",
    "    center = clusters.centers[clusters.predict(point)]\n",
    "    return sqrt(sum([x**2 for x in (point - center)]))\n",
    "\n",
    "\n",
    "# Save and load model\n",
    "def save_cluster_model(clusters, fn=\"test_model\"):\n",
    "    clusters.save(sc, \"target/org/apache/spark/PythonKMeansExample/KMeansModel\")\n",
    "\n",
    "def load_cluster_model(clusters, fn=\"test_model\"):\n",
    "    sameModel = KMeansModel.load(sc, fn)\n",
    "\n",
    "# def get_model(n_topics, cluster_model=\"kmeans\", cluster_params):\n",
    "#     if cluster_model == \"gaus\":\n",
    "#         gaussian_clustering()\n",
    "#         pass\n",
    "#     elif cluster_model == \"kmeans\":\n",
    "#         pass\n",
    "#     elif cluster_model == \"bimeans\":\n",
    "#         pass\n",
    "#     elif cluster_model == \"lda\":\n",
    "#         pass\n",
    "#     elif cluster_model == \"pic\":\n",
    "#         pass\n",
    "#     else:\n",
    "#         print(\"a viable option wasnt chosen\")\n",
    "    \n",
    "def grid_search(params, model):\n",
    "    for i in param:\n",
    "        rdd = get_svd()\n",
    "        model = get_model()\n",
    "        y_hats = make_preds()\n",
    "        results_to_disk(rdd, y)\n",
    "\n",
    "def gaussian_clustering(rdd):\n",
    "    # Build the model (cluster the data)\n",
    "    gmm = GaussianMixture.train(rdd, 2)\n",
    "\n",
    "    # Save and load model\n",
    "    gmm.save(sc, \"target/org/apache/spark/PythonGaussianMixtureExample/GaussianMixtureModel\")\n",
    "    sameModel = GaussianMixtureModel\\\n",
    "        .load(sc, \"target/org/apache/spark/PythonGaussianMixtureExample/GaussianMixtureModel\")\n",
    "\n",
    "    # output parameters of model\n",
    "    for i in range(2):\n",
    "        print(\"weight = \", gmm.weights[i], \"mu = \", gmm.gaussians[i].mu,\n",
    "              \"sigma = \", gmm.gaussians[i].sigma.toArray())\n",
    "\n",
    "\n",
    "def PIC_clustering(rdd, k, n_iters):\n",
    "    # Load and parse the data\n",
    "    pass\n",
    "    data = sc.textFile(\"data/mllib/pic_data.txt\")\n",
    "    similarities = data.map(lambda line: tuple([float(x) for x in line.split(' ')]))\n",
    "\n",
    "    # Cluster the data into two classes using PowerIterationClustering\n",
    "#     model = PowerIterationClustering.train(similarities, 2, 10)\n",
    "# # invalid syntax\n",
    "# #     model.assignments().foreach(lambda x: print(str(x.id) + \" -> \" + str(x.cluster)))\n",
    "\n",
    "#     # Save and load model\n",
    "#     model.save(sc, \"target/org/apache/spark/PythonPowerIterationClusteringExample/PICModel\")\n",
    "#     sameModel = Po[werIterationClusteringModel\\\n",
    "#             .load(sc, \"target/org/apache/spark/PythonPowerIterationClusteringExample/PICModel\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-33-3a597014084a>, line 29)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-33-3a597014084a>\"\u001b[1;36m, line \u001b[1;32m29\u001b[0m\n\u001b[1;33m    \"bimeans\":{\u001b[0m\n\u001b[1;37m            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "docConcentration or alpha, Concentration parameter (commonly named “alpha”) for the \n",
    "prior placed on documents’ distributions over topics (“theta”)\n",
    "\n",
    "A high alpha-value will lead to documents being more similar in terms of what topics they contain     \n",
    "The effect is based on topic distribution assumption\n",
    "if symmetric distribution - high alpha means that each doc will contain a mix of most topics\n",
    "if symmetric distribution - low alpha means that docs will contain a few topics\n",
    "if asymmetric distribution - vice versa\n",
    "--------------------\n",
    "\n",
    "topicConcentration or beta – Concentration parameter (commonly named “beta” or “eta”)\n",
    "for the prior placed on topics’ distributions over terms. (default: -1.0)\n",
    "\n",
    "A high beta-value will lead to topics being more similar in terms of what words they contain.    \n",
    "if symmetric distribution - high beta means that each doc will contain a mix of most words\n",
    "if symmetric distribution - low alpha means that docs will contain a few words\n",
    "if asymmetric distribution - vice versa\n",
    " \n",
    "\"\"\"\n",
    "cluster_model_params = {\n",
    "   \n",
    "    \"lda\":{\n",
    "        \"k\":[x for x in range(2, 20, 2)],\n",
    "        \"max_iters\": [x for x in range(15, 30, 10)], #default 20  \n",
    "        \"doc_con\":[float(x/100.0) for x in range(1, 10)], # default -1.0\n",
    "        \"topic_con\": [float(x/100.0) for x in range(1, 10)] # default -1.0\n",
    "    }\n",
    "    \"bimeans\":{\n",
    "        \"k\":[x for x in range(2, 20, 2)],\n",
    "        \"max_iters\": [x for x in range(15, 30, 10)], #default 20\n",
    "        \"minDivisibleClusterSize\": [float(x/100.0) for x in range(1, 10)], #percent\n",
    "    }\n",
    "    \"kmeans\":{\n",
    "        \"k\":[x for x in range(2, 20, 2)],\n",
    "        \"max_iters\": [x for x in range(15, 30, 10)], #default 20\n",
    "        \"initializationMode\": [\"random\", \"k-means||\")] #default k-means        \n",
    "    }\n",
    "    \"gaus\":{\n",
    "        \"k\":[x for x in range(2, 20, 2)],\n",
    "        \"max_iters\": [x for x in range(90, 150, 10)], #default 100\n",
    "    }\n",
    "    \"pic\":{\n",
    "        \"k\":[x for x in range(2, 20, 2)],\n",
    "        \"max_iters\": [x for x in range(90, 150, 10)], #default 100\n",
    "    }\n",
    "        \n",
    "}\n",
    "cluster_models = [\"lda\", \"bimeans\", \"kmeans\", \"gaus\", \"pic\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_bimeans_gs(rdd, verbose=True):\n",
    "    predictions = None\n",
    "    ks = [x for x in range(2, 20, 2)]\n",
    "    minDivisibleClusterSize: [float(x/100.0) for x in range(1, 50,10)], #percent\n",
    "    svd_topics = [x for x in range(2, 10, 2)]\n",
    "    for topic in svd_topics:\n",
    "        for k in ks:\n",
    "            # calculate tfidf scores\n",
    "            tfidf_rdd = get_tfidf(rdd)\n",
    "            \n",
    "            # transform bag of words to svd\n",
    "            svd = get_svd(tfidf_rdd, topic)\n",
    "            if verbose: print(svd)\n",
    "            \n",
    "            # run kmeans with left singular vectors \n",
    "            predictions_rdd = bimeans(svd, k)\n",
    "            if predictions is None:\n",
    "                predictions = predictions_rdd.collect()\n",
    "                print(type(predictions))\n",
    "            else:\n",
    "                predictions.append(predictions_rdd.collect())\n",
    "    kmeans_predictions_fn=\"predictions_k{}_topics{}.npz\"\n",
    "    save_cluster_predictions(np.array(predictions), model=\"km\", fn=kmeans_predictions_fn)\n",
    "    save_cluster_predictions(np.array(predictions), model=\"km\", fn=kmeans_predictions_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def transpose_rdd()\n",
    "# tfidf_rdd.flatMap(lambda x: x).take(3)\n",
    "# flatMap by keeping the column position\n",
    "# flat_rdd = tfidf_rdd.flatMap(lambda row: row.map(lambda col: (col, row.indexOf(col))))\n",
    "# flat_rdd.take(3)\n",
    "# .map(v => (v._2, v._1)) // key by column position\n",
    "# .groupByKey.sortByKey   // regroup on column position, thus all elements from the first column will be in the first row\n",
    "# .map(_._2)              // discard the key, keep only value\n",
    "# df = rdd.toDF()\n",
    "# # Grab data from first columns, since it will be transposed to new column headers\n",
    "# new_header = [i[0] for i in dt.select(\"_1\").rdd.map(tuple).collect()]\n",
    "\n",
    "# # Remove first column from dataframe\n",
    "# dt2 = dt.select([c for c in dt.columns if c not in ['_1']])\n",
    "\n",
    "# # Convert DataFrame to RDD\n",
    "# rdd = dt2.rdd.map(tuple)\n",
    "\n",
    "# # Transpose Data\n",
    "# rddT1 = rdd.zipWithIndex().flatMap(lambda (x,i): [(i,j,e) for (j,e) in enumerate(x)])\n",
    "# rddT2 = rddT1.map(lambda (i,j,e): (j, (i,e))).groupByKey().sortByKey()\n",
    "# rddT3 = rddT2.map(lambda (i, x): sorted(list(x), cmp=lambda (i1,e1),(i2,e2) : cmp(i1, i2)))\n",
    "# rddT4 = rddT3.map(lambda x: map(lambda (i, y): y , x))\n",
    "\n",
    "# # Convert back to DataFrame (along with header)\n",
    "# df = rddT4.toDF(new_header)\n",
    "\n",
    "# return df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
